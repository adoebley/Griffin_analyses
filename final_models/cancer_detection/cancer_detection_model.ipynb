{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to your data here\n",
    "\n",
    "#must be a tab separated text file with a column named 'sample' containing the unique sample name, and one column per feature for each of the 810 features output by Griffin for the TFBS 30,000 site analysis:\n",
    "#additional columns are optional and will be retained in the output but not used for anything else\n",
    "\n",
    "# example features:\n",
    "# central_coverage_AHR.hg38.30000     \n",
    "# mean_coverage_AHR.hg38.30000\n",
    "# amplitude_AHR.hg38.30000\n",
    "\n",
    "test_data_file = '../../lung_validation_cancer_detection/number_of_sites_analysis/merged_data/30000-sites_validation_reformatted.txt'\n",
    "outfile_name = 'cancer_detection_results.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "in_file = '../../lung_validation_cancer_detection/number_of_sites_analysis/merged_data/30000-sites_LUCAS_reformatted.txt'\n",
    "cval_file = '../../lung_validation_cancer_detection/number_of_sites_analysis/logreg_PCA_results/30000-sites_logreg_results/30000-sites.c_values.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 810\n",
      "Total samples: 287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "central_coverage_AHR.hg38.30000     -1.708273e-15\n",
       "central_coverage_AR.hg38.30000      -8.705386e-15\n",
       "central_coverage_ARNT.hg38.30000     3.682691e-15\n",
       "central_coverage_ARNTL.hg38.30000    5.567362e-15\n",
       "central_coverage_ASCL1.hg38.30000   -4.951517e-15\n",
       "                                         ...     \n",
       "mean_coverage_ZNF467.hg38.30000      3.922840e-14\n",
       "mean_coverage_ZNF554.hg38.30000     -4.439035e-14\n",
       "mean_coverage_ZNF580.hg38.30000     -4.097381e-15\n",
       "mean_coverage_ZNF770.hg38.30000      4.622551e-14\n",
       "mean_coverage_ZSCAN16.hg38.30000     4.857129e-15\n",
       "Length: 810, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import training data\n",
    "data = pd.read_csv(in_file, sep='\\t')\n",
    "data = data.set_index('sample')\n",
    "\n",
    "#get features and exclude all other columns\n",
    "griffin_features = data.columns[(data.columns.str.startswith('central_cov')) | (data.columns.str.startswith('mean_cov')) | (data.columns.str.startswith('amplitude'))]\n",
    "\n",
    "print('Features',len(griffin_features))\n",
    "\n",
    "data = data.sort_index()\n",
    "\n",
    "print('Total samples:',len(data))\n",
    "\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data[griffin_features])\n",
    "data[griffin_features] = scaler.transform(data[griffin_features])\n",
    "data[griffin_features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    385\n",
       "1     46\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import test data\n",
    "test_data = pd.read_csv(test_data_file, sep='\\t')\n",
    "test_data = test_data.set_index('sample')\n",
    "\n",
    "test_data = test_data.sort_index()\n",
    "\n",
    "#scale data\n",
    "test_data[griffin_features] = scaler.transform(test_data[griffin_features])\n",
    "test_data[griffin_features].mean()\n",
    "\n",
    "print(len(test_data))\n",
    "test_data['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[griffin_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components: 35\n",
      "best_c 0.01\n",
      "training accuracy 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "#perform PCA on the training set\n",
    "n_components = min(len(griffin_features), len(data))\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', random_state = 100)\n",
    "PCs = pca.fit_transform(data[griffin_features])\n",
    "principal_components = pd.DataFrame(data = PCs, columns = ['PC_'+str(m) for m in np.arange(n_components)], index = data.index)\n",
    "\n",
    "#find the principle components that make up 80% of the varience\n",
    "for j in range(len(pca.explained_variance_ratio_)):\n",
    "    current_sum = pca.explained_variance_ratio_[:j].sum()\n",
    "    if current_sum>=0.8:\n",
    "        break\n",
    "\n",
    "print('number of components:',j)\n",
    "pca_features = ['PC_'+str(m) for m in np.arange(0,j)]\n",
    "data = data.drop(columns = griffin_features).merge(principal_components[pca_features], left_index = True, right_index = True)\n",
    "\n",
    "#import cvalue\n",
    "cvals = pd.read_csv(cval_file,sep='\\t', header = None)\n",
    "best_c = cvals.mode().values[0][0]\n",
    "print('best_c',best_c)\n",
    "\n",
    "#train a  model on the full training dataset \n",
    "model = LogisticRegression(class_weight='balanced', max_iter=500, C=best_c)\n",
    "model.fit(data[pca_features], data['status'])\n",
    "\n",
    "#predict the training data\n",
    "pred = model.predict(data[pca_features])\n",
    "prob = model.predict_proba(data[pca_features])\n",
    "\n",
    "data['pred']= pred\n",
    "data['prob'] = prob[:,1]\n",
    "\n",
    "print('training accuracy',sum(data['status'] == data['pred'])/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply to the test data\n",
    "test_PCs = pca.transform(test_data[griffin_features])\n",
    "test_principal_components = pd.DataFrame(data = test_PCs , columns = ['PC_'+str(m) for m in np.arange(n_components)], index = test_data.index)\n",
    "\n",
    "test_data = test_data.drop(columns = griffin_features).merge(test_principal_components[pca_features], left_index = True, right_index = True)\n",
    "\n",
    "#predict the test data\n",
    "pred = model.predict(test_data[pca_features])\n",
    "prob = model.predict_proba(test_data[pca_features])\n",
    "\n",
    "test_data['prediction']= pred\n",
    "test_data['probability'] = prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(columns = pca_features)\n",
    "test_data.to_csv(outfile_name, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #output model in pickle format in case that is helpful to future users\n",
    "\n",
    "# import pickle\n",
    "# with open('cancer_detection_scaler.pkl', 'wb') as f:\n",
    "#     pickle.dump(scaler, f)\n",
    "\n",
    "# with open('cancer_detection_PCA.pkl', 'wb') as f:\n",
    "#     pickle.dump(pca, f)\n",
    "    \n",
    "# with open('cancer_detection_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
