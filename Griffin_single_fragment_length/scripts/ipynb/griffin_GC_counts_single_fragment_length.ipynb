{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##arguments for testing \n",
    "# %matplotlib inline\n",
    "\n",
    "# bam_file_path = '/fh/scratch/delete90/ha_g/realigned_bams/cfDNA_MBC_ULP_hg38/realign_bam_paired_snakemake-master/results/MBC_1041_1_ULP/MBC_1041_1_ULP_recalibrated.bam'\n",
    "# bam_file_name = 'MBC_1041_1_ULP'\n",
    "\n",
    "# # bam_file_path = 'test.bam.sorted.bam'\n",
    "# # bam_file_name = 'test'\n",
    "\n",
    "# mappable_regions_path = '/fh/fast/ha_g/user/adoebley/projects/griffin_revisions_1/genome/k100_minus_exclusion_lists.mappable_regions.hg38.bed'\n",
    "\n",
    "# ref_seq_path = '/fh/fast/ha_g/grp/reference/GRCh38/GRCh38.fa'\n",
    "# chrom_sizes_path = '/fh/fast/ha_g/grp/reference/GRCh38/hg38.standard.chrom.sizes'\n",
    "\n",
    "# out_dir = 'tmp'\n",
    "\n",
    "# map_q = 20\n",
    "# size_range = [15,500]\n",
    "# fragment_length = 165\n",
    "\n",
    "# CPU = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--bam_file', help='sample_bam_file', required=True)\n",
    "parser.add_argument('--bam_file_name', help='sample name (does not need to match actual file name)', required=True)\n",
    "parser.add_argument('--mappable_regions_path', help='highly mappable regions to be used in GC correction, bedGraph or bed foramt', required=True)\n",
    "\n",
    "parser.add_argument('--ref_seq',help='reference sequence (fasta format)',required=True)\n",
    "parser.add_argument('--chrom_sizes',help='path to chromosome sizes for the reference seq',required=True)\n",
    "\n",
    "parser.add_argument('--out_dir',help='folder for GC bias results',required=True)\n",
    "\n",
    "parser.add_argument('--map_q',help='minimum mapping quality for reads to be considered',type=int,required=True)\n",
    "parser.add_argument('--size_range',help='range of read sizes to be included',nargs=2, type=int, required=True)\n",
    "parser.add_argument('--fragment_length',help='most common fragment size',type=int, required=True)\n",
    "\n",
    "parser.add_argument('--CPU',help='number of CPU for parallelizing', type=int, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "bam_file_path = args.bam_file\n",
    "bam_file_name = args.bam_file_name\n",
    "mappable_regions_path=args.mappable_regions_path\n",
    "\n",
    "ref_seq_path = args.ref_seq\n",
    "chrom_sizes_path = args.chrom_sizes\n",
    "out_dir = args.out_dir\n",
    "\n",
    "map_q = args.map_q\n",
    "size_range = args.size_range\n",
    "fragment_length = args.fragment_length\n",
    "\n",
    "CPU = args.CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('arguments provided:')\n",
    "\n",
    "print('\\tbam_file_path = \"'+bam_file_path+'\"')\n",
    "print('\\tbam_file_name = \"'+bam_file_name+'\"')\n",
    "print('\\tmappable_regions_path = \"'+mappable_regions_path+'\"')\n",
    "\n",
    "print('\\tref_seq_path = \"'+ref_seq_path+'\"')\n",
    "print('\\tchrom_sizes_path = \"'+chrom_sizes_path+'\"')\n",
    "print('\\tout_dir = \"'+out_dir+'\"')\n",
    "\n",
    "print('\\tmap_q = '+str(map_q))\n",
    "print('\\tsize_range = '+str(size_range))\n",
    "print('\\tfragment_length = '+str(fragment_length))\n",
    "\n",
    "print('\\tCPU = '+str(CPU))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = out_dir +'/GC_counts/'+ bam_file_name+'.GC_counts.txt'\n",
    "\n",
    "print('out_file',out_file)\n",
    "\n",
    "#create a directory for the GC data\n",
    "if not os.path.exists(out_dir +'/GC_counts/'):\n",
    "    os.mkdir(out_dir +'/GC_counts/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import filter\n",
    "mappable_intervals = pd.read_csv(mappable_regions_path, sep='\\t', header=None)\n",
    "\n",
    "#remove non standard chromosomes and X and Y\n",
    "chroms = ['chr'+str(m) for m in range(1,23)]\n",
    "mappable_intervals = mappable_intervals[mappable_intervals[0].isin(chroms)]\n",
    "\n",
    "print('chroms:', chroms)\n",
    "print('number_of_intervals:',len(mappable_intervals))\n",
    "\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_reads(sublist):\n",
    "    #create a dict for holding the frequency of each read length and GC content\n",
    "    GC_dict = {}\n",
    "    GC_dict[fragment_length]={}\n",
    "    for num_GC in range(0,fragment_length+1):\n",
    "        GC_dict[fragment_length][num_GC]=0\n",
    "            \n",
    "#     for length in range(size_range[0],size_range[1]+1):\n",
    "#         GC_dict[length]={}\n",
    "#         for num_GC in range(0,length+1):\n",
    "#             GC_dict[length][num_GC]=0\n",
    "\n",
    "        \n",
    "    #import the bam file\n",
    "    #this needs to be done within the loop otherwise it gives a truncated file warning\n",
    "    bam_file = pysam.AlignmentFile(bam_file_path, \"rb\")\n",
    "    print('sublist intervals:',len(sublist))\n",
    "    \n",
    "    #this might also need to be in the loop\n",
    "    #import the ref_seq\n",
    "    ref_seq=pysam.FastaFile(ref_seq_path)\n",
    "    \n",
    "    for i in range(len(sublist)):\n",
    "        chrom = sublist.iloc[i][0]\n",
    "        start = sublist.iloc[i][1]\n",
    "        end = sublist.iloc[i][2]\n",
    "        if i%5000==0:\n",
    "            print('interval',i,':',chrom,start,end,'seconds:',np.round(time.time()-start_time))\n",
    "            sys.stdout.flush()\n",
    "        #fetch any read that overlaps the inteterval \n",
    "        fetched = bam_file.fetch(chrom,start,end)\n",
    "        for read in fetched:\n",
    "            #use both fw (positive template length) and rv (negative template length) reads\n",
    "            if (read.is_reverse==False and read.template_length>=size_range[0] and read.template_length<=size_range[1]) or \\\n",
    "            (read.is_reverse==True and -read.template_length>=size_range[0] and -read.template_length<=size_range[1]):\n",
    "                #qc filters, some longer fragments are considered 'improper pairs' but I would like to keep these\n",
    "                if read.is_paired==True and read.mapping_quality>=map_q and read.is_duplicate==False and read.is_qcfail==False:\n",
    "                    if read.is_reverse==False:\n",
    "                        fragment_start = read.reference_start\n",
    "                        fragment_end = read.reference_start+fragment_length #assume all fragments have the same length\n",
    "                    elif read.is_reverse==True:\n",
    "                        fragment_end = read.reference_start + read.reference_length\n",
    "                        fragment_start = fragment_end - fragment_length #assume all fragments have the same length\n",
    "\n",
    "                    #tally up the GC content\n",
    "                    fragment_seq = ref_seq.fetch(read.reference_name,fragment_start,fragment_end)\n",
    "                    fragment_seq=fragment_seq.upper()\n",
    "\n",
    "    #                 #################\n",
    "    #                 ##logic check####\n",
    "    #                 #################\n",
    "    #                 if read.is_reverse==False:\n",
    "    #                     if fragment_seq[0:read.reference_length]==read.query_sequence and len(fragment_seq)==read.template_length:\n",
    "    #                         print('fw match',read.reference_length)\n",
    "    #                     else:\n",
    "    #                         print(fragment_seq[0:read.reference_length],read.reference_length,'fw')\n",
    "    #                         print(read.query_sequence,len(read.query_sequence),'fw')\n",
    "    #                         print(len(fragment_seq),read.template_length)\n",
    "    #                         print('\\n')\n",
    "    #                 elif read.is_reverse==True:\n",
    "    #                     if fragment_seq[-read.reference_length:]==read.query_sequence and len(fragment_seq)==-read.template_length:\n",
    "    #                         print('rv match',read.reference_length)\n",
    "    #                     else:\n",
    "    #                         print(fragment_seq[-read.reference_length:],read.reference_length,'rv')\n",
    "    #                         print(read.query_sequence,len(read.query_sequence),'rv')\n",
    "    #                         print(len(fragment_seq),read.template_length)\n",
    "    #                         print('\\n')                        \n",
    "    #                 #################\n",
    "\n",
    "#                     #################\n",
    "#                     #second logic check\n",
    "#                     if read.is_reverse==False:\n",
    "#                         relative_start = fragment_start-start\n",
    "#                         min_relative_start_fw = min(relative_start,min_relative_start_fw)\n",
    "#                         if relative_start == min_relative_start_fw:\n",
    "#                             print('fw',chrom,start,end,fragment_start,relative_start)\n",
    "                    \n",
    "#                     else:\n",
    "#                         relative_start = fragment_start-start\n",
    "#                         min_relative_start_rv = min(relative_start,min_relative_start_rv)\n",
    "#                         if relative_start == min_relative_start_rv:\n",
    "#                             print('rv',chrom,start,end,fragment_start,relative_start)                            \n",
    "#                     #################\n",
    "                    \n",
    "                    \n",
    "                    #split and convert to numpy array\n",
    "                    fragment_seq = np.array(list(fragment_seq.replace('G','1').replace('C','1').replace('A','0').replace('T','0').replace('N','2')),dtype=int)\n",
    "\n",
    "                    #swap the 2 for a random 1 or 0 #there has to be a better way to do this but I can't figure it out\n",
    "                    #the 0 or 1 is required because the sliding window sum algorithm only does integers\n",
    "                    #unknown nucleotides should be quite rare if the filter is done correctly\n",
    "                    rng = np.random.default_rng(fragment_start)\n",
    "                    fragment_seq[fragment_seq==2] = rng.integers(2, size=len(fragment_seq[fragment_seq==2])) #random integer in range(2) (i.e. 0 or 1)\n",
    "\n",
    "                    num_GC = int(fragment_seq.sum())\n",
    "                    GC_dict[fragment_length][num_GC]+=1\n",
    "\n",
    "    print('done')\n",
    "    return(GC_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "p = Pool(processes=CPU) #use the available CPU\n",
    "sublists = np.array_split(mappable_intervals,CPU) #split the list into sublists, one per CPU\n",
    "\n",
    "GC_dict_list = p.map(collect_reads, sublists, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing only\n",
    "# start_time = time.time()\n",
    "# p = Pool(processes=CPU) #use the available CPU\n",
    "\n",
    "# sublists = np.array_split(mappable_intervals,CPU) #split the list into sublists, one per CPU\n",
    "# sublists = np.array_split(sublists[0],CPU) #additional split for testing\n",
    "\n",
    "# GC_dict_list = p.map(collect_reads, sublists, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_GC_df = pd.DataFrame()\n",
    "for i,GC_dict in enumerate(GC_dict_list):\n",
    "    GC_df = pd.DataFrame()\n",
    "    for length in GC_dict.keys():\n",
    "        current = pd.Series(GC_dict[length]).reset_index()\n",
    "        current = current.rename(columns={'index':'num_GC',0:'number_of_fragments'})\n",
    "        current['length']=length\n",
    "        current = current[['length','num_GC','number_of_fragments']]\n",
    "        GC_df = GC_df.append(current, ignore_index=True)\n",
    "    GC_df = GC_df.set_index(['length','num_GC'])\n",
    "    all_GC_df[i] = GC_df['number_of_fragments']\n",
    "    del(GC_df,GC_dict)\n",
    "    \n",
    "all_GC_df = all_GC_df.sum(axis=1)\n",
    "all_GC_df = pd.DataFrame(all_GC_df).rename(columns = {0:'number_of_fragments'})\n",
    "all_GC_df = all_GC_df.reset_index()\n",
    "all_GC_df.to_csv(out_file,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
