{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(in_file, status_col, min_tfx, min_cov):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    data = pd.read_csv(in_file, sep='\\t')\n",
    "    data = data.set_index('sample')\n",
    "\n",
    "    #get features and exclude all other columns\n",
    "    features = data.columns[(data.columns.str.startswith('central_cov')) | (data.columns.str.startswith('mean_cov')) | (data.columns.str.startswith('amplitude'))]\n",
    "    print('Features',len(features))\n",
    "\n",
    "    #get status\n",
    "    data['status'] = np.where(data[status_col]=='+',1,0)\n",
    "\n",
    "    #filter data\n",
    "    data = data[(data['tumor_fraction']>=min_tfx) & (data['ulp_wgs_coverage']>=min_cov)]\n",
    "    data = data.sort_index()\n",
    "\n",
    "    print('Total samples:',len(data))\n",
    "    \n",
    "#     display(pd.DataFrame(data[status_col].value_counts()))\n",
    "\n",
    "#     high_tfx = data[(data['tumor_fraction']>=0.1) & (data['ulp_wgs_coverage']>=min_cov)]\n",
    "#     print('High tfx samples:',len(high_tfx))\n",
    "#     display(pd.DataFrame(high_tfx[status_col].value_counts()))\n",
    "#     del(high_tfx)\n",
    "\n",
    "#     low_tfx = data[(data['tumor_fraction']<0.1) & (data['ulp_wgs_coverage']>=min_cov)]\n",
    "#     print('Low tfx samples:',len(low_tfx))\n",
    "#     display(pd.DataFrame(low_tfx[status_col].value_counts()))\n",
    "#     del(low_tfx)\n",
    "\n",
    "    #scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data[features])\n",
    "    data[features] = scaler.transform(data[features])\n",
    "    data[features].mean()\n",
    "    \n",
    "    return(data,features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrapping\n",
    "def run_bootstrap(data,iterations,features,report_interval):\n",
    "    import time\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from matplotlib import pyplot as plt\n",
    "    \n",
    "    hyperparameters = {'C': [0.00001, 0.0001, 0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    probabilities = pd.DataFrame(index=data.index)\n",
    "    c_vals = []\n",
    "    coefs = pd.DataFrame(index=features)\n",
    "\n",
    "    #loop for each iteration\n",
    "    countup = 0\n",
    "    for i in range(iterations):\n",
    "        if i%report_interval==0:\n",
    "            print(i, time.time()-start_time)\n",
    "            #prevent dfs from becoming too fragmented\n",
    "            probabilities = probabilities.copy()\n",
    "            coefs = coefs.copy()   \n",
    "            sys.stdout.flush() \n",
    "\n",
    "        patients = pd.Series(data['patient_id'].unique())\n",
    "\n",
    "        good_split = 0\n",
    "        while good_split == 0:\n",
    "            #bootstrap a training set with replacement\n",
    "            training_ids = patients.sample(len(patients), replace = True, random_state = 100+countup)\n",
    "\n",
    "            #get bootstrapped training set, if a patient ID is included in the training_ids set j times, include all samples from that patient j times\n",
    "            training = pd.DataFrame()\n",
    "            #group the training patient IDs by number of times they are observed in the bootstrapped training_ids\n",
    "            for j,df in pd.DataFrame(training_ids.value_counts().rename('count')).groupby(by = 'count'):\n",
    "\n",
    "                #identify the samples from this group of patients\n",
    "                current_data = data[data['patient_id'].isin(df.index)]\n",
    "\n",
    "                #copy the training samples so that they appear j times in training dataframe\n",
    "                current_training = pd.DataFrame()\n",
    "                for k in range(j):\n",
    "                    current_training = current_training.append(current_data)\n",
    "                training = training.append(current_training)\n",
    "\n",
    "\n",
    "            #the test set is all samples that aren't seen in the training data\n",
    "            test = data[~(data.index.isin(training.index))]\n",
    "\n",
    "            #print(len(training),len(training.index.unique()))\n",
    "            #print(len(test),len(test.index.unique()))\n",
    "\n",
    "            #check to make sure first time point low tumor fraction samples for both classes are included in the test set\n",
    "            if len(test[(test['tumor_fraction']<.1) & (test['first_passing_sample']==1)]['status'].unique())!=2:\n",
    "                print('Skipping',i)\n",
    "                countup += 1\n",
    "            else:\n",
    "                good_split = 1\n",
    "\n",
    "        #countup will get ahead of i if it has to skip bad train-test splits\n",
    "        countup +=1\n",
    "\n",
    "        #10 fold cross validation on the training set\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state =100+countup) \n",
    "\n",
    "        model = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "        search = GridSearchCV(estimator=model, param_grid=hyperparameters, cv=cv, n_jobs = 1)\n",
    "        search.fit(training[features], training['status'])\n",
    "        best_C = search.best_params_['C']\n",
    "\n",
    "        #train a new model on the full training dataset (is this the same as refit...?)\n",
    "        model = LogisticRegression(class_weight='balanced', max_iter=500, C=best_C)\n",
    "        model.fit(training[features], training['status'])\n",
    "\n",
    "        #predict the test data\n",
    "        pred = model.predict(test[features])\n",
    "        prob = model.predict_proba(test[features])\n",
    "\n",
    "\n",
    "        #collect metrics\n",
    "        current_output = pd.DataFrame(test[['status']])#.reset_index()\n",
    "        current_output['probability']=prob[:,1]\n",
    "        current_output['prediction']=pred\n",
    "        current_output['accuracy'] = np.where(current_output['prediction']==current_output['status'],1,0)\n",
    "  \n",
    "        #save results\n",
    "        probabilities[i] = current_output['probability']\n",
    "        c_vals.append(best_C)\n",
    "        coefs[i] = pd.Series(model.coef_[0], index = features)\n",
    "\n",
    "    probabilities = probabilities.merge(data[['status']], left_index=True, right_index=True)\n",
    "    \n",
    "    return(probabilities,c_vals,coefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_accuracy(probabilities,data,iterations):\n",
    "#     import numpy as np\n",
    "#     probabilities = probabilities.merge(data[['first_passing_sample','tumor_fraction']], left_index=True, right_index=True)\n",
    "    \n",
    "#     #high tfx per patient accuracy\n",
    "#     accuracy = []\n",
    "#     for i in range(iterations):\n",
    "#         current = probabilities[~(probabilities[i].isnull())][[i,'status','tumor_fraction','first_passing_sample']]\n",
    "#         current = current[(current['tumor_fraction']>=0.1) & (current['first_passing_sample']==1)]\n",
    "#         accuracy.append(sum(np.round(current[i])==current['status'])/len(current))\n",
    "\n",
    "#     high_tfx_accuracy = np.mean(accuracy)\n",
    "#     print('high tfx per patient:',np.round(np.mean(accuracy),3))\n",
    "    \n",
    "#     #low tfx per patient accuracy\n",
    "#     accuracy = []\n",
    "#     for i in range(iterations):\n",
    "#         current = probabilities[~(probabilities[i].isnull())][[i,'status','tumor_fraction','first_passing_sample']]\n",
    "#         current = current[(current['tumor_fraction']<0.1) & (current['first_passing_sample']==1)]\n",
    "#         accuracy.append(sum(np.round(current[i])==current['status'])/len(current))\n",
    "    \n",
    "#     low_tfx_accuracy = np.mean(accuracy)\n",
    "#     print('low tfx per patient:',np.round(np.mean(accuracy),3))\n",
    "    \n",
    "#     #per patient accuracy for all samples\n",
    "#     accuracy = []\n",
    "#     for i in range(iterations):\n",
    "#         current = probabilities[~(probabilities[i].isnull())][[i,'first_passing_sample','status']]\n",
    "#         current = current[(current['first_passing_sample']==1)]\n",
    "#         accuracy.append(sum(np.round(current[i])==current['status'])/len(current))\n",
    "\n",
    "#     all_tfx_accuracy = np.mean(accuracy)\n",
    "#     print('all tfx per patient:',np.round(all_tfx_accuracy,3))\n",
    "    \n",
    "#     return(high_tfx_accuracy,low_tfx_accuracy,all_tfx_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_AUC(probabilities,data,iterations):\n",
    "    #get AUC and accuracy for each bootstrap\n",
    "    from sklearn.metrics import roc_curve,auc\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    AUCs = pd.DataFrame()\n",
    "    accuracies = pd.DataFrame()\n",
    "\n",
    "    probabilities = probabilities.merge(data[['first_passing_sample','tumor_fraction']], left_index=True, right_index=True)\n",
    "    probabilities = probabilities[probabilities['first_passing_sample']==1]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        current_accuracy_dict = {}\n",
    "        current_AUC_dict = {}\n",
    "        \n",
    "        current = probabilities[~(probabilities[i].isnull())][['status','tumor_fraction',i]].copy()\n",
    "\n",
    "        current['accuracy'] = np.where(np.round(current[i])==current['status'],1,0)\n",
    "        \n",
    "        low_tfx = current[(current['tumor_fraction']<0.1)].copy()\n",
    "        high_tfx = current[(current['tumor_fraction']>=0.1)].copy()\n",
    "\n",
    "        for group,df in zip(['overall','high_tfx','low_tfx'],[current,high_tfx,low_tfx]):\n",
    "            fpr,tpr,_ = roc_curve(df['status'],df[i])\n",
    "\n",
    "            AUC = auc(fpr,tpr)\n",
    "            current_AUC_dict[group] = AUC\n",
    "\n",
    "            accuracy = sum(np.round(df[i])==df['status'])/len(df)\n",
    "            current_accuracy_dict[group] = accuracy\n",
    "            \n",
    "        accuracies = accuracies.append(pd.Series(current_accuracy_dict), ignore_index=True)\n",
    "        AUCs = AUCs.append(pd.Series(current_AUC_dict), ignore_index=True)\n",
    "        \n",
    "\n",
    "    AUC_CIs = pd.DataFrame([AUCs.median(), AUCs.quantile(.025), AUCs.quantile(.975)]).T\n",
    "    AUC_CIs = AUC_CIs.rename(columns = {'Unnamed 0':'median'})    \n",
    "\n",
    "    accuracy_CIs = pd.DataFrame([accuracies.median(), accuracies.quantile(.025), accuracies.quantile(.975)]).T\n",
    "    accuracy_CIs = accuracy_CIs.rename(columns = {'Unnamed 0':'median'})    \n",
    "\n",
    "    return(accuracies,accuracy_CIs,AUCs,AUC_CIs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
