{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import pybedtools\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml \n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "##sample specific params for testing\n",
    "sample_name = 'HD45.ctDNA.WGS.FC19269447'\n",
    "bam_path = '/fh/scratch/delete90/ha_g/realigned_bams/cfDNA_deepWGS_hg38/deepWGS_fastq_to_bam_paired_snakemake/results/HD45.ctDNA.WGS.FC19269447/HD45.ctDNA.WGS.FC19269447_recalibrated.bam'\n",
    "GC_bias_path = '../GC_correction/deepWGS_GC_and_mappability_correction/results/GC_bias/HD45.ctDNA.WGS.FC19269447.GC_bias.txt'\n",
    "mappability_bias_path = '../GC_correction/deepWGS_GC_and_mappability_correction/results/mappability_bias/HD45.ctDNA.WGS.FC19269447.mappability_bias.txt'\n",
    "\n",
    "tmp_dir = 'tmp'\n",
    "\n",
    "ref_seq_path = '/fh/fast/ha_g/grp/reference/GRCh38/GRCh38.fa'\n",
    "mappability_bw='../genome/k100.Umap.MultiTrackMappability.hg38.bw'\n",
    "chrom_sizes_path = '/fh/fast/ha_g/grp/reference/GRCh38/hg38.standard.chrom.sizes'\n",
    "\n",
    "# #additional params for testing\n",
    "sites_yaml = 'test_sites.yaml'\n",
    "griffin_scripts_dir = '../Griffin/scripts/'\n",
    "\n",
    "chrom_column = 'Chrom'\n",
    "position_column = 'position'\n",
    "strand_column = 'Strand'\n",
    "chroms = ['chr'+str(m) for m in np.arange(1,23)]\n",
    "\n",
    "norm_window = [-5000, 5000] \n",
    "sz_range = [100, 200]\n",
    "map_q = 20\n",
    "\n",
    "number_of_sites = 'none'\n",
    "sort_by = 'none'\n",
    "ascending = 'none'\n",
    "\n",
    "CPU = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--sample_name', help='name of sample', required=True)\n",
    "# parser.add_argument('--bam', help='bam file', required=True)\n",
    "# parser.add_argument('--GC_bias', help='GC bias info from griffin_GC_bias', required=True)\n",
    "# parser.add_argument('--mappability_bias', help='mappability bias info from griffin_mappability_bias', required=True)\n",
    "\n",
    "# parser.add_argument('--tmp_dir', help = 'directory for temporary outputs (may be large)', required=True)\n",
    "\n",
    "# parser.add_argument('--reference_genome',help = 'path to the reference genome',required=True)\n",
    "# parser.add_argument('--mappability_bw',help = 'bigWig file of genome wide mappability scores',required=True)\n",
    "# parser.add_argument('--chrom_sizes_path', help='path to chrom sizes file', required=True)\n",
    "\n",
    "# parser.add_argument('--sites_yaml', help='.bed file of sites', required=True)\n",
    "# parser.add_argument('--griffin_scripts_dir', help='path/to/scripts/', required=True)\n",
    "\n",
    "# parser.add_argument('--chrom_column',help='name of column containing chromosome number', default='Chrom')\n",
    "# parser.add_argument('--position_column',help='name of column containing chromosome position', default='Chrom')\n",
    "# parser.add_argument('--strand_column',help='name of column containing the strand (+ or -)', default='Strand')\n",
    "# parser.add_argument('--chroms', help='chroms to include when selecting sites', nargs='*', default=['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22'])\n",
    "\n",
    "# parser.add_argument('--norm_window',help='start and end of the window to be used for normalization',nargs=2, type=int, default=(-5000,5000))\n",
    "# parser.add_argument('--size_range',help='acceptable size range for fragments (to filter out genomic contamination)',nargs=2, type=int, default=(0,500))\n",
    "# parser.add_argument('--map_quality',help='minimum mapping quality', type=int, default=60)\n",
    "\n",
    "# parser.add_argument('--number_of_sites',help='number of sites to analyze', default='NA')\n",
    "# parser.add_argument('--sort_by',help='how to select the sites to analyze', default='none')\n",
    "# parser.add_argument('--ascending',help='whether to sort in ascending or descending order when selecting sites', default='NA')\n",
    "\n",
    "# parser.add_argument('--CPU',help='cpu available for parallelizing', type = int, required = True)\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# sample_name = args.sample_name\n",
    "# bam_path = args.bam\n",
    "# GC_bias_path = args.GC_bias\n",
    "# mappability_bias_path = args.mappability_bias\n",
    "\n",
    "# tmp_dir = args.tmp_dir \n",
    "\n",
    "# ref_seq_path = args.reference_genome\n",
    "# mappability_bw = args.mappability_bw\n",
    "# chrom_sizes_path = args.chrom_sizes_path\n",
    "\n",
    "# sites_yaml=args.sites_yaml\n",
    "# griffin_scripts_dir = args.griffin_scripts_dir\n",
    "\n",
    "# chrom_column = args.chrom_column\n",
    "# position_column=args.position_column\n",
    "# strand_column=args.strand_column\n",
    "# chroms = args.chroms\n",
    "\n",
    "# norm_window =args.norm_window\n",
    "# sz_range=args.size_range\n",
    "# map_q=args.map_quality\n",
    "\n",
    "# number_of_sites=args.number_of_sites\n",
    "# sort_by=args.sort_by\n",
    "# ascending=args.ascending\n",
    "\n",
    "# CPU = args.CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parameters:\n",
      "\tsample_name = \"HD45.ctDNA.WGS.FC19269447\"\n",
      "\tbam_path = \"/fh/scratch/delete90/ha_g/realigned_bams/cfDNA_deepWGS_hg38/deepWGS_fastq_to_bam_paired_snakemake/results/HD45.ctDNA.WGS.FC19269447/HD45.ctDNA.WGS.FC19269447_recalibrated.bam\"\n",
      "\tGC_bias_path = \"../GC_correction/deepWGS_GC_and_mappability_correction/results/GC_bias/HD45.ctDNA.WGS.FC19269447.GC_bias.txt\"\n",
      "\tmappability_bias_path = \"../GC_correction/deepWGS_GC_and_mappability_correction/results/mappability_bias/HD45.ctDNA.WGS.FC19269447.mappability_bias.txt\"\n",
      "\ttmp_dir = \"tmp\"\n",
      "\tref_seq_path = \"/fh/fast/ha_g/grp/reference/GRCh38/GRCh38.fa\"\n",
      "\tmappability_bw = \"../genome/k100.Umap.MultiTrackMappability.hg38.bw\"\n",
      "\tchrom_sizes_path = \"/fh/fast/ha_g/grp/reference/GRCh38/hg38.standard.chrom.sizes\"\n",
      "\tsites_yaml = \"/fh/fast/ha_g/user/adoebley/projects/griffin_revisions_1/midpoints_vs_coverage/test_sites.yaml\"\n",
      "\tgriffin_scripts_dir = \"../Griffin/scripts/\"\n",
      "\tchrom_column = \"Chrom\"\n",
      "\tposition_column = \"position\"\n",
      "\tstrand_column = \"Strand\"\n",
      "\tchroms =  ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22']\n",
      "\tnorm_window =  [-5000, 5000]\n",
      "\tsz_range = [100, 200]\n",
      "\tmap_q = 20\n",
      "\tnumber_of_sites = \"none\"\n",
      "\tsort_by = \"none\"\n",
      "\tascending = \"none\"\n",
      "\tCPU = 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print parameters for easy troubleshooting\n",
    "if ascending.lower()=='false':\n",
    "    ascending=False\n",
    "elif ascending.lower()=='true':\n",
    "    ascending=True\n",
    "else:\n",
    "    ascending='none'\n",
    "    \n",
    "print('\\nparameters:')\n",
    "\n",
    "print('\\tsample_name = \"'+sample_name+'\"')\n",
    "print('\\tbam_path = \"'+bam_path+'\"')\n",
    "print('\\tGC_bias_path = \"'+GC_bias_path+'\"')\n",
    "print('\\tmappability_bias_path = \"'+mappability_bias_path+'\"')\n",
    "\n",
    "print('\\ttmp_dir = \"'+tmp_dir+'\"')\n",
    "\n",
    "print('\\tref_seq_path = \"'+ref_seq_path+'\"')\n",
    "print('\\tmappability_bw = \"'+mappability_bw+'\"')\n",
    "print('\\tchrom_sizes_path = \"'+chrom_sizes_path+'\"')\n",
    "\n",
    "print('\\tsites_yaml = \"'+os.path.abspath(sites_yaml)+'\"')\n",
    "print('\\tgriffin_scripts_dir = \"'+griffin_scripts_dir+'\"')\n",
    "\n",
    "print('\\tchrom_column = \"'+chrom_column+'\"')\n",
    "print('\\tposition_column = \"'+position_column+'\"')\n",
    "print('\\tstrand_column = \"'+strand_column+'\"')\n",
    "print('\\tchroms = ',chroms)\n",
    "\n",
    "print('\\tnorm_window = ', norm_window)\n",
    "print('\\tsz_range =',sz_range)\n",
    "print('\\tmap_q =',map_q)\n",
    "\n",
    "print('\\tnumber_of_sites = \"'+str(number_of_sites)+'\"')\n",
    "print('\\tsort_by = \"'+sort_by+'\"')\n",
    "print('\\tascending = \"'+str(ascending)+'\"')\n",
    "\n",
    "print('\\tCPU =',CPU)\n",
    "print('\\n')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using smoothed map bias\n"
     ]
    }
   ],
   "source": [
    "#define global parameters and open global files\n",
    "########################################\n",
    "#GET GC BIAS\n",
    "########################################\n",
    "#open the GC_bias file \n",
    "GC_bias = pd.read_csv(GC_bias_path, sep='\\t')\n",
    "\n",
    "#get rid of extremely low GC bias values\n",
    "#these fragments will now be excluded \n",
    "#these fragments are extremely rare so it is difficult to get a good estimate of GC bias\n",
    "GC_bias['smoothed_GC_bias'] = np.where(GC_bias['smoothed_GC_bias']<0.05,np.nan,GC_bias['smoothed_GC_bias'])\n",
    "\n",
    "GC_bias = GC_bias[['length','num_GC','smoothed_GC_bias']]\n",
    "GC_bias = GC_bias.set_index(['num_GC','length']).unstack()\n",
    "\n",
    "#convert to a dictionary\n",
    "GC_bias = GC_bias.to_dict()\n",
    "\n",
    "#get rid of values where the num_GC is greater than the length (included due to the way I made the dict)\n",
    "GC_bias2 = {}\n",
    "for key in GC_bias.keys():\n",
    "    length = key[1]\n",
    "    GC_bias2[length] = {}\n",
    "    for num_GC in range(0,length+1):\n",
    "        bias = GC_bias[key][num_GC]\n",
    "        GC_bias2[length][num_GC]=bias\n",
    "GC_bias = GC_bias2 \n",
    "del(GC_bias2)\n",
    "\n",
    "########################################\n",
    "#GET mappability bias\n",
    "########################################\n",
    "mappability_bias = pd.read_csv(mappability_bias_path, sep='\\t')\n",
    "mappability_bias['mappable_percent'] = np.round(mappability_bias['mappable_percent']).astype(int) #convert indexes to integers\n",
    "mappability_bias = mappability_bias[['mappable_percent','smoothed_map_bias']].set_index('mappable_percent').to_dict()['smoothed_map_bias']\n",
    "print('using smoothed map bias')\n",
    "def closest_key(dictionary, i):\n",
    "    sorted_keys=np.array(sorted(dictionary.keys()))\n",
    "    idx = (np.abs(sorted_keys - i)).argmin() \n",
    "    closest_key=sorted_keys[idx]\n",
    "    return(closest_key)\n",
    "\n",
    "#depending on the read length, some values might be missing, add them\n",
    "for i in range(0,101):\n",
    "    if i in mappability_bias:\n",
    "        pass\n",
    "    else:\n",
    "        mappability_bias[i] = mappability_bias[closest_key(mappability_bias,i)]\n",
    "        print('adding mappabilty',i,'to dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snakemake should create these folders, but if not using the snakemake, this is needed\n",
    "tmp_sample_dir = tmp_dir+'/'+sample_name\n",
    "if not os.path.exists(tmp_sample_dir): \n",
    "    os.mkdir(tmp_sample_dir)\n",
    "\n",
    "tmp_pybedtools = tmp_sample_dir+'/tmp_pybedtools'\n",
    "if not os.path.exists(tmp_pybedtools): \n",
    "    os.mkdir(tmp_pybedtools)\n",
    "pybedtools.set_tempdir(tmp_pybedtools)\n",
    "\n",
    "tmp_bigWig = tmp_sample_dir+'/tmp_bigWig'\n",
    "if not os.path.exists(tmp_bigWig): \n",
    "    os.mkdir(tmp_bigWig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the griffin scripts\n",
    "sys.path.insert(0, griffin_scripts_dir)\n",
    "import griffin_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYL1.hg38.10000 processing all 10000 sites\n"
     ]
    }
   ],
   "source": [
    "#import the site_lists\n",
    "with open(sites_yaml,'r') as f:\n",
    "    sites = yaml.safe_load(f)\n",
    "sites = sites['site_lists']\n",
    "\n",
    "all_sites = pd.DataFrame()\n",
    "for site_name in sites.keys():\n",
    "    site_file = sites[site_name]\n",
    "    current_sites = griffin_functions.import_and_filter_sites(site_name,site_file,strand_column,chrom_column,position_column,chroms,ascending,sort_by,number_of_sites)\n",
    "    all_sites = all_sites.append(current_sites, ignore_index=True).copy()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sites (fw/rv/undirected/total): 0/0/10000/10000\n"
     ]
    }
   ],
   "source": [
    "#number of bp to fetch upstream and downstream of the site\n",
    "upstream_bp = norm_window[0]-sz_range[0] #this should be negative\n",
    "downstream_bp = norm_window[1]+sz_range[0] #this should be positive\n",
    "all_sites = griffin_functions.define_fetch_interval('Total sites',all_sites,chrom_column,position_column,\n",
    "                                                    chroms,chrom_sizes_path,upstream_bp,downstream_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervals to fetch:\t8308\n",
      "Total bp to fetch:\t91408752\n",
      "Max fetch length: 53925 bp\n"
     ]
    }
   ],
   "source": [
    "#convert to pybedtools and merge overlapping segments\n",
    "start_time = time.time()\n",
    "all_sites_bed = pybedtools.BedTool.from_dataframe(all_sites[[chrom_column,'fetch_start','fetch_end']])\n",
    "all_sites_bed = all_sites_bed.sort()\n",
    "all_sites_bed = all_sites_bed.merge()\n",
    "print('Intervals to fetch:\\t'+str(len(all_sites_bed)))\n",
    "print('Total bp to fetch:\\t'+str(all_sites_bed.total_coverage()))\n",
    "sys.stdout.flush()\n",
    "\n",
    "#split the long intervals \n",
    "to_fetch = all_sites_bed.to_dataframe()\n",
    "to_fetch['length'] = to_fetch['end']-to_fetch['start']\n",
    "\n",
    "print('Max fetch length: '+str(to_fetch['length'].max())+' bp')\n",
    "\n",
    "to_fetch = to_fetch[['chrom','start','end']]\n",
    "to_fetch = to_fetch.sort_values(by=['chrom','start']).reset_index(drop=True)\n",
    "to_fetch = to_fetch.reset_index() #add an index column\n",
    "split_len = pybedtools.BedTool.from_dataframe(to_fetch[['chrom','start','end']]).total_coverage()\n",
    "\n",
    "# print('Intervals to fetch (after splitting long intervals):\\t'+str(len(to_fetch)))\n",
    "# print('Total bp to fetch (after splitting long intervals):\\t'+str(split_len))\n",
    "sys.stdout.flush()\n",
    "pybedtools.cleanup(verbose=False, remove_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fragments(input_list):\n",
    "    i,chrom,start,end = input_list\n",
    "    #open the bam file for each pool worker (otherwise individual pool workers can close it)\n",
    "    bam_file = pysam.AlignmentFile(bam_path)\n",
    "    \n",
    "    #open the ref seq\n",
    "    ref_seq=pysam.FastaFile(ref_seq_path)\n",
    "    mappability = pyBigWig.open(mappability_bw)    \n",
    "    \n",
    "    #make dicts to hold the fetched positions\n",
    "    columns = np.arange(start,end,1)\n",
    "    cov_dict={m:0 for m in columns} \n",
    "    GC_cov_dict={m:0 for m in columns} \n",
    "    GC_map_cov_dict={m:0 for m in columns} \n",
    "    \n",
    "    #fetch reads\n",
    "    fetched=bam_file.fetch(contig=chrom, start=start, stop=end) #fetch reads that map to the region of interest\n",
    "                \n",
    "    ########################\n",
    "    #count coverage\n",
    "    ########################\n",
    "    for read in fetched:\n",
    "        #filter out reads\n",
    "        if abs(read.template_length)>=sz_range[0] and abs(read.template_length)<=sz_range[1] \\\n",
    "        and read.is_paired==True and read.mapping_quality>=map_q and read.is_duplicate==False and read.is_qcfail==False:\n",
    "            #only use fw reads with positive fragment lengths (negative indicates an abnormal pair)\n",
    "            #all paired end reads have a fw and rv read so we don't need the rv read to find the midpoint.\n",
    "            if read.is_reverse==False and read.template_length>0:\n",
    "                fragment_start = read.reference_start #for fw read, read start is fragment start\n",
    "                fragment_end = read.reference_start+read.template_length\n",
    "                midpoint = int(np.floor((fragment_start+fragment_end)/2))\n",
    "                                \n",
    "                #count the GC content\n",
    "                fragment_seq = ref_seq.fetch(read.reference_name,fragment_start,fragment_end)\n",
    "                fragment_seq = np.array(list(fragment_seq.upper()))\n",
    "                fragment_seq[np.isin(fragment_seq, ['A','T','W'])] = 0\n",
    "                fragment_seq[np.isin(fragment_seq, ['C','G','S'])] = 1\n",
    "                rng = np.random.default_rng(fragment_start)\n",
    "                fragment_seq[np.isin(fragment_seq, ['N','R','Y','K','M','B','D','H','V'])] = rng.integers(2, size=len(fragment_seq[np.isin(fragment_seq, ['N','R','Y','K','M','B','D','H','V'])])) #random integer in range(2) (i.e. 0 or 1)\n",
    "                fragment_seq = fragment_seq.astype(float)\n",
    "    \n",
    "                #find the two read locations for mappability correction\n",
    "                fw_read_map = mappability.values(chrom,read.reference_start,read.reference_start+read.reference_length)\n",
    "                fw_read_map = np.mean(np.nan_to_num(fw_read_map)) #replace any nan with zero and take the mean\n",
    "\n",
    "                rv_read_map = mappability.values(chrom,read.reference_start+read.template_length-read.reference_length,read.reference_start+read.template_length)\n",
    "                rv_read_map = np.mean(np.nan_to_num(rv_read_map)) #replace any nan with zero and take the mean\n",
    "                                \n",
    "                #check that the site is in the window          \n",
    "                if midpoint>=start and midpoint<end:\n",
    "                    #count the fragment\n",
    "                    cov_dict[midpoint]+=1\n",
    "\n",
    "                    ##get the GC bias\n",
    "                    read_GC_content = sum(fragment_seq)\n",
    "                    read_GC_bias = GC_bias[abs(read.template_length)][read_GC_content]\n",
    "                    \n",
    "                    #get the mappability bias\n",
    "                    read_map = np.int32(np.round(100*(fw_read_map+rv_read_map)/2))\n",
    "                    read_map_bias = mappability_bias[read_map]\n",
    "\n",
    "                    #count the fragment weighted by GC bias\n",
    "                    if not np.isnan(read_GC_bias):\n",
    "                        GC_cov_dict[midpoint]+=(1/read_GC_bias)\n",
    "                        GC_map_cov_dict[midpoint]+=(1/read_GC_bias)*(1/read_map_bias)\n",
    "                    #print(read_GC_bias,read_map,read_map_bias)\n",
    "                else: #if fragment doesn't fully overlap\n",
    "                    #print('end outside')\n",
    "                    continue\n",
    "                del(read,midpoint,fw_read_map,rv_read_map,fragment_seq)\n",
    "                \n",
    "            else:\n",
    "                #print('reverse',read.is_reverse)\n",
    "                continue\n",
    "                \n",
    "    output = pd.DataFrame(pd.Series(cov_dict, name = 'uncorrected'))\n",
    "    output['GC_corrected'] = pd.Series(GC_cov_dict)\n",
    "    output['GC_map_corrected'] = pd.Series(GC_map_cov_dict)\n",
    "    output = output[output['uncorrected']>0] #don't waste memory on positions with no coverage\n",
    "    output['chrom'] = chrom\n",
    "    output = output.reset_index().rename(columns = {'index':'position'})\n",
    "    output['uncorrected'] = output['uncorrected'].astype(float)\n",
    "    output['GC_corrected'] = np.round(output['GC_corrected'],5)\n",
    "    output['GC_map_corrected'] = np.round(output['GC_map_corrected'],5)\n",
    "\n",
    "    bam_file.close()\n",
    "    ref_seq.close()\n",
    "    mappability.close()\n",
    "    \n",
    "    if (i+1)%100==0:\n",
    "        printout = griffin_functions.progress_report([chrom,start,end],'intervals',start_time,time.time(),i,len(to_fetch))\n",
    "        print(printout)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch\n",
      "chr1_21763341_21773541: 100 of 8308 intervals done in 0 min 6 sec, 9 min 15 sec remaining\n",
      "chr1_36452925_36463125: 200 of 8308 intervals done in 0 min 12 sec, 8 min 39 sec remaining\n",
      "chr1_62267110_62277310: 300 of 8308 intervals done in 0 min 19 sec, 8 min 28 sec remaining\n",
      "chr1_107936257_107946457: 400 of 8308 intervals done in 0 min 24 sec, 8 min 6 sec remaining\n",
      "chr1_155971209_155981409: 500 of 8308 intervals done in 0 min 31 sec, 8 min 14 sec remaining\n",
      "chr1_182479677_182489877: 600 of 8308 intervals done in 0 min 39 sec, 8 min 24 sec remaining\n",
      "chr1_206376059_206386259: 700 of 8308 intervals done in 0 min 45 sec, 8 min 12 sec remaining\n",
      "chr1_230419727_230435840: 800 of 8308 intervals done in 0 min 51 sec, 8 min 6 sec remaining\n",
      "chr10_6143419_6153619: 900 of 8308 intervals done in 0 min 57 sec, 7 min 53 sec remaining\n",
      "chr10_30856347_30867818: 1000 of 8308 intervals done in 1 min 3 sec, 7 min 43 sec remaining\n",
      "chr10_72330020_72340220: 1100 of 8308 intervals done in 1 min 10 sec, 7 min 39 sec remaining\n",
      "chr10_97864451_97874651: 1200 of 8308 intervals done in 1 min 16 sec, 7 min 32 sec remaining\n",
      "chr10_133553582_133563782: 1300 of 8308 intervals done in 1 min 22 sec, 7 min 24 sec remaining\n",
      "chr11_35927762_35937962: 1400 of 8308 intervals done in 1 min 28 sec, 7 min 16 sec remaining\n",
      "chr11_65412436_65427266: 1500 of 8308 intervals done in 1 min 34 sec, 7 min 8 sec remaining\n",
      "chr11_110984864_110995064: 1600 of 8308 intervals done in 1 min 40 sec, 7 min 2 sec remaining\n",
      "chr12_4148593_4158793: 1700 of 8308 intervals done in 1 min 46 sec, 6 min 55 sec remaining\n",
      "chr12_32054156_32064356: 1800 of 8308 intervals done in 1 min 52 sec, 6 min 47 sec remaining\n",
      "chr12_71231451_71241651: 1900 of 8308 intervals done in 1 min 59 sec, 6 min 43 sec remaining\n",
      "chr12_110855891_110866091: 2000 of 8308 intervals done in 2 min 7 sec, 6 min 41 sec remaining\n",
      "chr13_21048328_21058528: 2100 of 8308 intervals done in 2 min 15 sec, 6 min 39 sec remaining\n",
      "chr13_50507623_50517823: 2200 of 8308 intervals done in 2 min 22 sec, 6 min 36 sec remaining\n",
      "chr13_100230741_100240941: 2300 of 8308 intervals done in 2 min 29 sec, 6 min 30 sec remaining\n",
      "chr14_50056810_50067010: 2400 of 8308 intervals done in 2 min 37 sec, 6 min 28 sec remaining\n",
      "chr14_83080266_83090466: 2500 of 8308 intervals done in 2 min 45 sec, 6 min 23 sec remaining\n",
      "chr15_31261490_31271690: 2600 of 8308 intervals done in 2 min 52 sec, 6 min 18 sec remaining\n",
      "chr15_65814023_65824223: 2700 of 8308 intervals done in 2 min 59 sec, 6 min 13 sec remaining\n",
      "chr15_85095576_85105776: 2800 of 8308 intervals done in 3 min 7 sec, 6 min 9 sec remaining\n",
      "chr16_3097219_3107419: 2900 of 8308 intervals done in 3 min 15 sec, 6 min 3 sec remaining\n",
      "chr16_31254810_31265010: 3000 of 8308 intervals done in 3 min 22 sec, 5 min 57 sec remaining\n",
      "chr16_70423246_70436118: 3100 of 8308 intervals done in 3 min 32 sec, 5 min 56 sec remaining\n",
      "chr16_89965074_89975274: 3200 of 8308 intervals done in 3 min 40 sec, 5 min 52 sec remaining\n",
      "chr17_28514353_28524553: 3300 of 8308 intervals done in 3 min 49 sec, 5 min 47 sec remaining\n",
      "chr17_50122736_50132936: 3400 of 8308 intervals done in 3 min 57 sec, 5 min 42 sec remaining\n",
      "chr17_75262122_75272322: 3500 of 8308 intervals done in 4 min 5 sec, 5 min 37 sec remaining\n",
      "chr18_9745717_9764069: 3600 of 8308 intervals done in 4 min 14 sec, 5 min 32 sec remaining\n",
      "chr18_60651653_60661853: 3700 of 8308 intervals done in 4 min 21 sec, 5 min 25 sec remaining\n",
      "chr19_7332732_7343067: 3800 of 8308 intervals done in 4 min 28 sec, 5 min 18 sec remaining\n",
      "chr19_18962807_18977598: 3900 of 8308 intervals done in 4 min 36 sec, 5 min 12 sec remaining\n",
      "chr19_48870296_48883960: 4000 of 8308 intervals done in 4 min 43 sec, 5 min 5 sec remaining\n",
      "chr2_12502438_12512638: 4100 of 8308 intervals done in 4 min 50 sec, 4 min 58 sec remaining\n",
      "chr2_43008912_43019919: 4200 of 8308 intervals done in 4 min 58 sec, 4 min 52 sec remaining\n",
      "chr2_70176380_70186580: 4300 of 8308 intervals done in 5 min 6 sec, 4 min 45 sec remaining\n",
      "chr2_109290204_109300404: 4400 of 8308 intervals done in 5 min 13 sec, 4 min 38 sec remaining\n",
      "chr2_148541406_148551606: 4500 of 8308 intervals done in 5 min 20 sec, 4 min 31 sec remaining\n",
      "chr2_195482101_195492301: 4600 of 8308 intervals done in 5 min 28 sec, 4 min 24 sec remaining\n",
      "chr2_231163845_231174045: 4700 of 8308 intervals done in 5 min 35 sec, 4 min 17 sec remaining\n",
      "chr20_13744914_13755114: 4800 of 8308 intervals done in 5 min 42 sec, 4 min 10 sec remaining\n",
      "chr20_44581452_44591652: 4900 of 8308 intervals done in 5 min 50 sec, 4 min 3 sec remaining\n",
      "chr21_14515493_14525693: 5000 of 8308 intervals done in 5 min 59 sec, 3 min 57 sec remaining\n",
      "chr21_44909216_44919416: 5100 of 8308 intervals done in 6 min 6 sec, 3 min 50 sec remaining\n",
      "chr22_36855839_36867581: 5200 of 8308 intervals done in 6 min 13 sec, 3 min 43 sec remaining\n",
      "chr3_10421948_10432148: 5300 of 8308 intervals done in 6 min 21 sec, 3 min 36 sec remaining\n",
      "chr3_39245132_39255332: 5400 of 8308 intervals done in 6 min 28 sec, 3 min 29 sec remaining\n",
      "chr3_58642831_58653031: 5500 of 8308 intervals done in 6 min 35 sec, 3 min 22 sec remaining\n",
      "chr3_120637651_120647851: 5600 of 8308 intervals done in 6 min 42 sec, 3 min 14 sec remaining\n",
      "chr3_151885677_151895877: 5700 of 8308 intervals done in 6 min 49 sec, 3 min 7 sec remaining\n",
      "chr3_190978955_190989155: 5800 of 8308 intervals done in 6 min 57 sec, 3 min 0 sec remaining\n",
      "chr4_14901935_14912135: 5900 of 8308 intervals done in 7 min 4 sec, 2 min 53 sec remaining\n",
      "chr4_61873012_61883212: 6000 of 8308 intervals done in 7 min 13 sec, 2 min 46 sec remaining\n",
      "chr4_108528549_108538749: 6100 of 8308 intervals done in 7 min 21 sec, 2 min 39 sec remaining\n",
      "chr4_169075295_169085495: 6200 of 8308 intervals done in 7 min 29 sec, 2 min 32 sec remaining\n",
      "chr5_31840480_31850680: 6300 of 8308 intervals done in 7 min 36 sec, 2 min 25 sec remaining\n",
      "chr5_76300405_76310605: 6400 of 8308 intervals done in 7 min 45 sec, 2 min 18 sec remaining\n",
      "chr5_126753604_126763804: 6500 of 8308 intervals done in 7 min 52 sec, 2 min 11 sec remaining\n",
      "chr5_157323363_157333563: 6600 of 8308 intervals done in 8 min 4 sec, 2 min 5 sec remaining\n",
      "chr6_5029532_5039732: 6700 of 8308 intervals done in 8 min 12 sec, 1 min 58 sec remaining\n",
      "chr6_30825558_30835758: 6800 of 8308 intervals done in 8 min 19 sec, 1 min 50 sec remaining\n",
      "chr6_51908403_51918603: 6900 of 8308 intervals done in 8 min 26 sec, 1 min 43 sec remaining\n",
      "chr6_113876577_113886777: 7000 of 8308 intervals done in 8 min 33 sec, 1 min 35 sec remaining\n",
      "chr6_151943420_151953620: 7100 of 8308 intervals done in 8 min 40 sec, 1 min 28 sec remaining\n",
      "chr7_6170759_6180959: 7200 of 8308 intervals done in 8 min 47 sec, 1 min 21 sec remaining\n",
      "chr7_42092809_42103009: 7300 of 8308 intervals done in 8 min 55 sec, 1 min 13 sec remaining\n",
      "chr7_98677047_98687247: 7400 of 8308 intervals done in 9 min 2 sec, 1 min 6 sec remaining\n",
      "chr7_139848546_139860804: 7500 of 8308 intervals done in 9 min 10 sec, 0 min 59 sec remaining\n",
      "chr8_20195905_20206105: 7600 of 8308 intervals done in 9 min 18 sec, 0 min 52 sec remaining\n",
      "chr8_52153864_52182251: 7700 of 8308 intervals done in 9 min 27 sec, 0 min 44 sec remaining\n",
      "chr8_98231225_98241425: 7800 of 8308 intervals done in 9 min 33 sec, 0 min 37 sec remaining\n",
      "chr8_132763608_132773808: 7900 of 8308 intervals done in 9 min 41 sec, 0 min 30 sec remaining\n",
      "chr9_21042463_21052663: 8000 of 8308 intervals done in 9 min 48 sec, 0 min 22 sec remaining\n",
      "chr9_91080609_91090809: 8100 of 8308 intervals done in 9 min 56 sec, 0 min 15 sec remaining\n",
      "chr9_122214383_122224583: 8200 of 8308 intervals done in 10 min 4 sec, 0 min 7 sec remaining\n",
      "chr9_136693112_136703312: 8300 of 8308 intervals done in 10 min 11 sec, 0 min 0 sec remaining\n",
      "Done with fetch 10 min 13 sec\n"
     ]
    }
   ],
   "source": [
    "#run the analysis \n",
    "print('Starting fetch')\n",
    "sys.stdout.flush()\n",
    "start_time = time.time()\n",
    "\n",
    "p = Pool(processes=CPU) #use the specified number of processes\n",
    "results = p.map(collect_fragments, to_fetch.values, 1) #Send only one interval to each processor at a time.\n",
    "\n",
    "elapsed_time = time.time()-overall_start_time\n",
    "print('Done with fetch '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "del(elapsed_time)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting export\n",
      "Done with export 10 min 58 sec\n"
     ]
    }
   ],
   "source": [
    "print('Starting export')\n",
    "sys.stdout.flush()\n",
    "start_time = time.time()\n",
    "\n",
    "chrom_sizes = pd.read_csv(chrom_sizes_path, sep='\\t', header=None)\n",
    "chrom_sizes = chrom_sizes[chrom_sizes[0].isin(chroms)]\n",
    "\n",
    "uncorrected_bw = pyBigWig.open(tmp_bigWig+\"/\"+sample_name+\".midpoint.uncorrected.bw\", \"w\")\n",
    "GC_bw = pyBigWig.open(tmp_bigWig+\"/\"+sample_name+\".midpoint.GC_corrected.bw\", \"w\")\n",
    "GC_map_bw = pyBigWig.open(tmp_bigWig+\"/\"+sample_name+\".midpoint.GC_map_corrected.bw\", \"w\")\n",
    "\n",
    "uncorrected_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "GC_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "GC_map_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "\n",
    "for i in range(len(results)):\n",
    "    current = results[i]\n",
    "    if len(current)>0:\n",
    "        if np.nansum(current['uncorrected'])>0:\n",
    "            uncorrected_bw.addEntries(list(current['chrom']), list(current['position']), ends = list(current['position']+1), values = list(current['uncorrected']))  \n",
    "        else:\n",
    "            print('no uncorrected reads:')\n",
    "            print(current)\n",
    "            \n",
    "        if np.nansum(current['GC_corrected'])>0:\n",
    "            GC_bw.addEntries(list(current['chrom']), list(current['position']), ends = list(current['position']+1), values = list(current['GC_corrected'])) \n",
    "        else:\n",
    "            print('no GC corrected reads:')\n",
    "            print(current)\n",
    "            \n",
    "        if np.nansum(current['GC_map_corrected'])>0:   \n",
    "            GC_map_bw.addEntries(list(current['chrom']), list(current['position']), ends = list(current['position']+1), values = list(current['GC_map_corrected']))  \n",
    "        else:\n",
    "            print('no GC map corrected reads:')\n",
    "            print(current)\n",
    "            \n",
    "    if (i+1)%10000==0 and len(current)>0:\n",
    "        printout = griffin_functions.progress_report(list(current.iloc[0][['chrom','position']]),'intervals',start_time,time.time(),i,len(results))\n",
    "        print(printout)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "uncorrected_bw.close()\n",
    "GC_bw.close()\n",
    "GC_map_bw.close()\n",
    "\n",
    "elapsed_time = time.time()-overall_start_time\n",
    "print('Done with export '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
