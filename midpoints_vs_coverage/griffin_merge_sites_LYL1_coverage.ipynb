{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import zscore\n",
    "import yaml \n",
    "from multiprocessing import Pool\n",
    "import pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_name = 'HD45.ctDNA.WGS.FC19269447'\n",
    "uncorrected_bw_path = 'tmp/HD45.ctDNA.WGS.FC19269447/tmp_bigWig/HD45.ctDNA.WGS.FC19269447.fragment.uncorrected.bw'\n",
    "GC_corrected_bw_path = 'tmp/HD45.ctDNA.WGS.FC19269447/tmp_bigWig/HD45.ctDNA.WGS.FC19269447.fragment.GC_corrected.bw'\n",
    "GC_map_corrected_bw_path = 'tmp/HD45.ctDNA.WGS.FC19269447/tmp_bigWig/HD45.ctDNA.WGS.FC19269447.fragment.GC_map_corrected.bw'\n",
    "\n",
    "tmp_dir = 'tmp'\n",
    "results_dir = 'results'\n",
    "\n",
    "mappability_bw='../genome/k100.Umap.MultiTrackMappability.hg38.bw'\n",
    "chrom_sizes_path = '/fh/fast/ha_g/grp/reference/GRCh38/hg38.standard.chrom.sizes'\n",
    "\n",
    "# #additional params for testing\n",
    "sites_yaml = 'test_sites.yaml'\n",
    "\n",
    "griffin_scripts_dir = '../Griffin/scripts/'\n",
    "\n",
    "chrom_column = 'Chrom'\n",
    "position_column = 'position'\n",
    "strand_column = 'Strand'\n",
    "chroms = ['chr'+str(m) for m in np.arange(1,23)]\n",
    "\n",
    "norm_window = [-5000, 5000] #for testing\n",
    "\n",
    "save_window = [-1000, 1000]#for testing\n",
    "center_window = [-30,30] #define the center of the interval for feature calculation\n",
    "fft_window = [-960,960]\n",
    "fft_index = 10\n",
    "smoothing_length = 167 #fragment_length\n",
    "\n",
    "encode_exclude = '../genome/encode_unified_GRCh38_exclusion_list.bed'\n",
    "centromere_path = '../genome/hg38_centromeres.bed'\n",
    "gap_path = '../genome/hg38_gaps.bed'\n",
    "patch_path = '../genome/hg38_fix_patches.bed'\n",
    "alternative_haplotype_path = '../genome/hg38_alternative_haplotypes.bed'\n",
    "\n",
    "exclude_paths = [encode_exclude,centromere_path,gap_path,patch_path,alternative_haplotype_path]\n",
    "del(encode_exclude,centromere_path,gap_path,patch_path)\n",
    "\n",
    "\n",
    "step = 15\n",
    "\n",
    "CNA_normalization = 'False'\n",
    "individual = 'False'\n",
    "smoothing = 'True'\n",
    "\n",
    "exclude_outliers_parameter = 'True'\n",
    "exclude_zero_mappability_parameter = 'True'\n",
    "\n",
    "number_of_sites = 'none'\n",
    "sort_by = 'none'\n",
    "ascending = 'none'\n",
    "\n",
    "CPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--sample_name', help='name of sample', required=True)\n",
    "# parser.add_argument('--uncorrected_bw_path', help='uncorrected bigWig from griffin_coverage', required=True)\n",
    "# parser.add_argument('--GC_corrected_bw_path', help='GC_corrected bigWig from griffin_coverage', required=True)\n",
    "# parser.add_argument('--GC_map_corrected_bw_path', help='GC_corrected bigWig from griffin_coverage', required=True)\n",
    "\n",
    "# parser.add_argument('--tmp_dir', help = 'directory for temporary outputs (may be large)', required=True)\n",
    "# parser.add_argument('--results_dir', help = 'directory for results', required=True)\n",
    "\n",
    "# parser.add_argument('--mappability_bw',help = 'bigWig file of genome wide mappability scores',required=True)\n",
    "# parser.add_argument('--chrom_sizes_path', help='path to chrom sizes file', required=True)\n",
    "\n",
    "# parser.add_argument('--sites_yaml', help='.bed file of sites', required=True)\n",
    "# parser.add_argument('--griffin_scripts_dir', help='path/to/scripts/', required=True)\n",
    "\n",
    "# parser.add_argument('--chrom_column',help='name of column containing chromosome number', default='Chrom')\n",
    "# parser.add_argument('--position_column',help='name of column containing chromosome position', default='Chrom')\n",
    "# parser.add_argument('--strand_column',help='name of column containing the strand (+ or -)', default='Strand')\n",
    "# parser.add_argument('--chroms', help='chroms to include when selecting sites', nargs='*', default=['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22'])\n",
    "\n",
    "# parser.add_argument('--norm_window',help='start and end of the window to be used for normalization',nargs=2, type=int, default=(-5000,5000))\n",
    "# parser.add_argument('--save_window',help='start and end of the window to be saved in the outputs',nargs=2, type=int, default=(-1000,1000))\n",
    "# parser.add_argument('--center_window',help='start and end of the window to be used for calculating the central coverage feature',nargs=2, type=int, default=(-1000,1000))\n",
    "# parser.add_argument('--fft_window',help='start and end of the window to be used for calculating the amplitude feature (default is for bin size 15)',nargs=2, type=int, default=(-960,960))\n",
    "# parser.add_argument('--fft_index',help='index of the fft component to be saved as the amplitude feature (default is for 15bp bins and -960 to 960 fft_window)', type = int, default=10)\n",
    "# parser.add_argument('--smoothing_length',help='window length for the Savitzky-Golay smoothing function (should be approximately the mean fragment length)', type = int, default=165)\n",
    "\n",
    "# parser.add_argument('--exclude_paths', help='path to bed files of regions to filter out (excluded regions, centromeres, gaps, patches, alternative haplotypes), or \"none\" to not exclude any regions', required=True, nargs = '*')\n",
    "\n",
    "# parser.add_argument('--step',help='step size when calculating coverage', type=int, default=5)\n",
    "\n",
    "# parser.add_argument('--CNA_normalization',help='whether to normalize each site individually to the copy number within the normalization window',default='False', required = True)\n",
    "# parser.add_argument('--individual',help='save individual site coverage. TRUE WILL RESULT IN HUGE OUTPUT FILES. (True/False)',default='False', required = True)\n",
    "# parser.add_argument('--smoothing',help='whether to use a savgol filter to smooth sites (True/False)', default = 'True', required = True)\n",
    "\n",
    "# parser.add_argument('--exclude_outliers',help='whether to exclude bins with extreme outlier coverage >10SD above the mean (True/False)', default='True', required = True)\n",
    "# parser.add_argument('--exclude_zero_mappability',help='whether to exclude bins with zero mappability (True/False)', default='True', required = True)\n",
    "\n",
    "# parser.add_argument('--number_of_sites',help='number of sites to analyze', default='none')\n",
    "# parser.add_argument('--sort_by',help='how to select the sites to analyze', default='none')\n",
    "# parser.add_argument('--ascending',help='whether to sort in ascending or descending order when selecting sites', default='none')\n",
    "\n",
    "# parser.add_argument('--CPU',help='CPU available for parallelizing', type = int, required = True)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# sample_name = args.sample_name\n",
    "# uncorrected_bw_path = args.uncorrected_bw_path\n",
    "# GC_corrected_bw_path = args.GC_corrected_bw_path\n",
    "# GC_map_corrected_bw_path = args.GC_map_corrected_bw_path\n",
    "\n",
    "# tmp_dir = args.tmp_dir\n",
    "# results_dir = args.results_dir\n",
    "\n",
    "# mappability_bw = args.mappability_bw\n",
    "# chrom_sizes_path = args.chrom_sizes_path\n",
    "\n",
    "# sites_yaml = args.sites_yaml\n",
    "# griffin_scripts_dir = args.griffin_scripts_dir\n",
    "# chrom_column = args.chrom_column\n",
    "# position_column = args.position_column\n",
    "# strand_column = args.strand_column\n",
    "# chroms = args.chroms\n",
    "\n",
    "# norm_window = args.norm_window\n",
    "# save_window = args.save_window\n",
    "# center_window = args.center_window\n",
    "# fft_window = args.fft_window\n",
    "# fft_index = args.fft_index\n",
    "# smoothing_length = args.smoothing_length\n",
    "\n",
    "# exclude_paths = args.exclude_paths\n",
    "\n",
    "# step = args.step\n",
    "\n",
    "# CNA_normalization = args.CNA_normalization\n",
    "# individual = args.individual\n",
    "# smoothing = args.smoothing\n",
    "\n",
    "# exclude_outliers_parameter = args.exclude_outliers\n",
    "# exclude_zero_mappability_parameter = args.exclude_zero_mappability\n",
    "\n",
    "# number_of_sites = args.number_of_sites\n",
    "# sort_by = args.sort_by\n",
    "# ascending = args.ascending\n",
    "\n",
    "# CPU = args.CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization window (rounded down to step): [-4995, 4995]\n",
      "Saving window (rounded down to step): [-990, 990]\n",
      "Center window (rounded down to step): [-30, 30]\n",
      "FFT window (rounded down to step): [-960, 960]\n",
      "Savgol filter smoothing window: 165\n",
      "Ascending is: none\n"
     ]
    }
   ],
   "source": [
    "norm_window=[int(np.ceil(norm_window[0]/step)*step),int(np.floor(norm_window[1]/step)*step)] #round to the nearest step inside the window\n",
    "save_window=[int(np.ceil(save_window[0]/step)*step),int(np.floor(save_window[1]/step)*step)] #round to the nearest step inside the window\n",
    "center_window=[int(np.ceil(center_window[0]/step)*step),int(np.floor(center_window[1]/step)*step)] #round to the nearest step inside the window\n",
    "fft_window=[int(np.ceil(fft_window[0]/step)*step),int(np.floor(fft_window[1]/step)*step)] #round to the nearest step inside the window\n",
    "\n",
    "all_positions = np.arange(norm_window[0],norm_window[1])\n",
    "norm_columns = np.arange(norm_window[0],norm_window[1],step)\n",
    "save_columns = np.arange(save_window[0],save_window[1],step)\n",
    "center_columns = np.arange(center_window[0],center_window[1],step)\n",
    "fft_columns = np.arange(fft_window[0],fft_window[1],step)\n",
    "\n",
    "smoothing_length=int(np.round(smoothing_length/step)*step) #round fragment length to the nearest step\n",
    "\n",
    "if ascending.lower()=='false':\n",
    "    ascending=False\n",
    "elif ascending.lower()=='true':\n",
    "    ascending=True\n",
    "else:\n",
    "    ascending='none'\n",
    "    \n",
    "print('Normalization window (rounded down to step):', norm_window)\n",
    "print('Saving window (rounded down to step):', save_window)\n",
    "print('Center window (rounded down to step):', center_window)\n",
    "print('FFT window (rounded down to step):', fft_window)\n",
    "print('Savgol filter smoothing window:', smoothing_length)\n",
    "print('Ascending is:',ascending)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding regions: ['../genome/encode_unified_GRCh38_exclusion_list.bed', '../genome/hg38_centromeres.bed', '../genome/hg38_gaps.bed', '../genome/hg38_fix_patches.bed', '../genome/hg38_alternative_haplotypes.bed']\n",
      "Excluding bins with coverage outliers: True\n",
      "Excluding bins with zero mappability: True\n"
     ]
    }
   ],
   "source": [
    "print('Excluding regions:',exclude_paths)\n",
    "print('Excluding bins with coverage outliers:',exclude_outliers_parameter)\n",
    "print('Excluding bins with zero mappability:',exclude_zero_mappability_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snakemake should create these folders, but if not using the snakemake, this is needed\n",
    "tmp_sample_dir = tmp_dir+'/'+sample_name\n",
    "if not os.path.exists(tmp_sample_dir): \n",
    "    os.mkdir(tmp_sample_dir)\n",
    "\n",
    "tmp_pybedtools = tmp_sample_dir+'/tmp_pybedtools2'\n",
    "if not os.path.exists(tmp_pybedtools): \n",
    "    os.mkdir(tmp_pybedtools)\n",
    "pybedtools.set_tempdir(tmp_pybedtools)\n",
    "\n",
    "tmp_bigWig = tmp_sample_dir+'/tmp_bigWig'\n",
    "if not os.path.exists(tmp_bigWig): \n",
    "    os.mkdir(tmp_bigWig)\n",
    "\n",
    "#make results dir\n",
    "results_sample_dir = results_dir+'/'+sample_name\n",
    "if not os.path.exists(results_sample_dir): \n",
    "    os.mkdir(results_sample_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the griffin scripts\n",
    "sys.path.insert(0, griffin_scripts_dir)\n",
    "import griffin_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding: ../genome/encode_unified_GRCh38_exclusion_list.bed\n",
      "excluding: ../genome/hg38_centromeres.bed\n",
      "excluding: ../genome/hg38_gaps.bed\n",
      "excluding: ../genome/hg38_fix_patches.bed\n",
      "excluding: ../genome/hg38_alternative_haplotypes.bed\n"
     ]
    }
   ],
   "source": [
    "if exclude_paths==['none']:\n",
    "    print('No excluded regions.')\n",
    "\n",
    "else: #if there are regions to be excluded\n",
    "    #get the excluded regions\n",
    "    merged_exclude_regions = pybedtools.BedTool('\\n', from_string=True)\n",
    "\n",
    "    #create an empty bed file\n",
    "    excluded_regions_bw = pyBigWig.open(tmp_bigWig+\"/excluded_regions.bw\", \"w\")\n",
    "    chrom_sizes = pd.read_csv(chrom_sizes_path, sep='\\t', header=None)\n",
    "    chrom_sizes = chrom_sizes[chrom_sizes[0].isin(chroms)]\n",
    "    excluded_regions_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "\n",
    "    for path in exclude_paths:\n",
    "        print('excluding:',path)\n",
    "        current_regions = pybedtools.BedTool(path)\n",
    "        merged_exclude_regions = merged_exclude_regions.cat(current_regions)    \n",
    "        del(current_regions)\n",
    "    merged_exclude_regions = merged_exclude_regions.to_dataframe()\n",
    "    merged_exclude_regions = merged_exclude_regions[merged_exclude_regions['chrom'].isin(chroms)]\n",
    "    pybedtools.cleanup()\n",
    "    excluded_regions_bw.addEntries(list(merged_exclude_regions['chrom']), list(merged_exclude_regions['start']), ends = list(merged_exclude_regions['end']), values = [1.0 for m in range(len(merged_exclude_regions))])  \n",
    "\n",
    "    excluded_regions_bw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 1 site lists\n"
     ]
    }
   ],
   "source": [
    "#import the site_lists\n",
    "with open(sites_yaml,'r') as f:\n",
    "    sites = yaml.safe_load(f)\n",
    "sites = sites['site_lists']\n",
    "print('Analyzing '+str(len(sites))+' site lists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bw_values(bw_path,current_sites,site_name,name):\n",
    "    elapsed_time = time.time()-overall_start_time\n",
    "    print(site_name+' '+name+' starting fetch '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "    del(elapsed_time)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    bw = pyBigWig.open(bw_path)\n",
    "    \n",
    "    fw_markers = ['+',1,'1']\n",
    "    rv_markers = ['-',-1,'-1']\n",
    "\n",
    "    results = pd.DataFrame(np.zeros([len(current_sites),norm_window[1]-norm_window[0]]))\n",
    "    start_time = time.time()\n",
    "        \n",
    "    for i in range(len(current_sites)):\n",
    "        chrom,start,end,strand = current_sites.iloc[i][[chrom_column,'fetch_start','fetch_end',strand_column]]\n",
    "\n",
    "        values = bw.values(chrom, start, end, numpy=True)\n",
    "        values = np.nan_to_num(values) #turn nan into zero because bw doesn't store zero\n",
    "        \n",
    "        #if the window extends beyond the end of the chromosome add np.nan to fill the gap\n",
    "        if len(values)<(norm_window[1]-norm_window[0]):\n",
    "            ###################\n",
    "            position = current_sites.iloc[i][position_column]\n",
    "            array_start = position + norm_window[0]\n",
    "            array_end = position + norm_window[1]\n",
    "\n",
    "            temp_series = pd.Series(np.full(norm_window[1]-norm_window[0], np.nan), index = np.arange(array_start,array_end))\n",
    "            temp_series[np.arange(start,end)] = values\n",
    "            \n",
    "            #print('too_short',i,len(values),norm_window[1]-norm_window[0])\n",
    "            #print(chrom,start,end,strand)\n",
    "            #print(values)\n",
    "            \n",
    "            values = temp_series.values\n",
    "            del(temp_series, position, array_start,array_end)\n",
    "            ###################\n",
    "        if strand in rv_markers:\n",
    "            values = values[::-1]\n",
    "        results.iloc[i] =  pd.Series(values)\n",
    "        \n",
    "        if (i+1)%10000==0:\n",
    "            printout = griffin_functions.progress_report([site_name,name,chrom,start,end],'intervals',start_time,time.time(),i,len(current_sites))\n",
    "            print(printout,', size',np.round(sys.getsizeof(results)/(1024**3),2),'GB')\n",
    "            sys.stdout.flush()\n",
    "    results.columns = np.arange(norm_window[0],norm_window[1])\n",
    "    \n",
    "    printout = griffin_functions.progress_report([site_name,name,'fetch_complete'],'intervals',start_time,time.time(),i,len(current_sites))\n",
    "    print(printout,', size',np.round(sys.getsizeof(results)/(1024**3),2),'GB')\n",
    "    sys.stdout.flush()\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_bins(results,name):\n",
    "    #If a bin has an np.nan value, the whole bin will become np.nan\n",
    "    summed = np.sum(results.values.reshape(len(results),int(len(results.columns)/step), step), axis = 2)    \n",
    "    summed = pd.DataFrame(summed)\n",
    "    summed.columns = norm_columns\n",
    "     \n",
    "    return(summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_regions(results,excluded_regions,name):\n",
    "    results = np.where(excluded_regions>0,np.nan,results)#if any bp in the bin were excluded (1) exclude the bin\n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = norm_columns\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_zero_mappability(results,mappability_values,name):\n",
    "    results = np.where(mappability_values>0,results,np.nan)#only retain positions where the mappability is >0\n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = norm_columns\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outlier_mask(results,site_name):\n",
    "    max_value = results.max().max()\n",
    "    min_cutoff = 2 #minimum coverage that must be retained even if it is an outlier\n",
    "    print(site_name,'max_bin_coverage is',max_value, 'midpoints')\n",
    "    \n",
    "    scores = pd.DataFrame(zscore(results.values, axis = None, nan_policy='omit'))\n",
    "    outlier_mask = pd.DataFrame(np.where(scores<10,1,np.nan))\n",
    "    outlier_mask.columns = norm_columns\n",
    "    \n",
    "    \n",
    "    if (results*outlier_mask).max().max()<min_cutoff:\n",
    "        print(site_name, 'low coverage, resetting the outlier cutoff to '+str(min_cutoff))\n",
    "        outlier_mask = pd.DataFrame(np.where(results<=min_cutoff,1,np.nan))\n",
    "        outlier_mask.columns = norm_columns\n",
    "        \n",
    "    outlier_cutoff = (results*outlier_mask).max().max()\n",
    "    print(site_name,'masking sites with  >', outlier_cutoff, 'midpoints')\n",
    "\n",
    "    return(outlier_mask,outlier_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_smooth(results,site_name,name):\n",
    "    #get the mean midpoints per valid position in each site\n",
    "    mean_reads_per_bp_in_normalization_window = np.nanmean(results[norm_columns],axis = 1)/step\n",
    "    mean_reads_per_bp_in_saved_window = np.nanmean(results[save_columns],axis = 1)/step\n",
    "        \n",
    "    #normalize individual sites to 1 to remove CNA\n",
    "    if CNA_normalization.lower() == 'true':\n",
    "        print(site_name,name,'normalizing CNAs')\n",
    "        mean_data = np.nanmean(results.values,axis = 1, keepdims=True)\n",
    "        #replace zero with nan so there aren't any infinities in the output\n",
    "        mean_data = np.where(mean_data==0,np.nan,mean_data)\n",
    "        results[norm_columns] = results[norm_columns]/mean_data  \n",
    "        \n",
    "    #take the mean of all sites\n",
    "    if not individual.lower()=='true':\n",
    "        print(site_name,name,'averaging sites')\n",
    "        results = pd.DataFrame(pd.Series(np.nanmean(results[norm_columns], axis = 0), index=norm_columns)).T\n",
    "        results.columns = norm_columns\n",
    "        mean_reads_per_bp_in_normalization_window = np.nanmean(mean_reads_per_bp_in_normalization_window)\n",
    "        mean_reads_per_bp_in_saved_window = np.nanmean(mean_reads_per_bp_in_saved_window)\n",
    "        \n",
    "    #smooth the sites\n",
    "    if smoothing.lower()=='true':\n",
    "        print(site_name,name,'smoothing')\n",
    "        #savgol window should be approx one fragment length but it must be odd\n",
    "        savgol_window=np.floor(smoothing_length/step)\n",
    "        if savgol_window%2==0:\n",
    "            savgol_window=savgol_window+1\n",
    "        savgol_window=int(savgol_window)\n",
    "\n",
    "        results[norm_columns] = savgol_filter(results[norm_columns], savgol_window, 3)\n",
    "    \n",
    "    #normalize the average site to 1\n",
    "    print(site_name,name,'correcting for read depth')\n",
    "    mean_value = np.nanmean(results[norm_columns])\n",
    "    results[norm_columns] = results[norm_columns]/mean_value\n",
    "    \n",
    "    #save only plot columns \n",
    "    results = results[save_columns].copy()\n",
    "    \n",
    "    results['mean_reads_per_bp_in_normalization_window'] = mean_reads_per_bp_in_normalization_window\n",
    "    results['mean_reads_per_bp_in_saved_window'] = mean_reads_per_bp_in_saved_window\n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(results):\n",
    "    results['mean_coverage'] = results[save_columns].mean(axis = 1, skipna=False) #pandas skips na by default\n",
    "    results['central_coverage'] = results[center_columns].mean(axis = 1, skipna=False)\n",
    "    fft_res = np.fft.fft(results[fft_columns])\n",
    "    results['amplitude'] = np.abs(fft_res[:,fft_index])\n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sites(input_list):\n",
    "    site_name,site_file = input_list\n",
    "    #get the site lists and define the fetch interval\n",
    "    current_sites = griffin_functions.import_and_filter_sites(site_name,site_file,strand_column,chrom_column,position_column,chroms,ascending,sort_by,number_of_sites)   \n",
    "    current_sites = griffin_functions.define_fetch_interval(site_name,current_sites,chrom_column,position_column,chroms,chrom_sizes_path,norm_window[0],norm_window[1])\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    #dict to hold results \n",
    "    results_dict = {'uncorrected': {'input_path':uncorrected_bw_path},\n",
    "                    'GC_corrected': {'input_path':GC_corrected_bw_path},\n",
    "                    'GC_map_corrected': {'input_path':GC_map_corrected_bw_path}}\n",
    "\n",
    "    #fetch coverage and sum into bins of length step\n",
    "    for key in results_dict.keys():\n",
    "        results_dict[key]['coverage'] = fetch_bw_values(results_dict[key]['input_path'],current_sites,site_name,key)\n",
    "        results_dict[key]['coverage'] = sum_bins(results_dict[key]['coverage'],key)\n",
    "    \n",
    "    #exclude specified regions\n",
    "    if not exclude_paths == ['none']:\n",
    "        print(site_name+' - excluding specified regions.')\n",
    "        regions_to_exclude = fetch_bw_values(tmp_bigWig+\"/excluded_regions.bw\",current_sites,site_name,'to_exclude')\n",
    "        regions_to_exclude = sum_bins(regions_to_exclude,'to_exclude')\n",
    "        for key in results_dict.keys():\n",
    "            results_dict[key]['coverage'] = exclude_regions(results_dict[key]['coverage'],regions_to_exclude,key)\n",
    "        del(regions_to_exclude)\n",
    "\n",
    "    #exclude zero mappability\n",
    "    if exclude_zero_mappability_parameter.lower()=='true':\n",
    "        print(site_name+' - excluding zero mappability.')\n",
    "        #fetch excluded_regions\n",
    "        mappability_values = fetch_bw_values(mappability_bw,current_sites,site_name,'mappabilty')  \n",
    "        #replace zero with np.nan for mappability\n",
    "        #when summing bins, any bin with one or more zeros will now be np.nan\n",
    "        mappability_values[all_positions] = np.where(mappability_values[all_positions]==0,np.nan,mappability_values[all_positions])\n",
    "        mappability_values = sum_bins(mappability_values,'mappability')\n",
    "        \n",
    "        for key in results_dict.keys():\n",
    "            results_dict[key]['coverage'] = exclude_zero_mappability(results_dict[key]['coverage'],mappability_values,key)\n",
    "        del(mappability_values)\n",
    "\n",
    "    #mask out bins with coverage >10 SD above the mean\n",
    "    if exclude_outliers_parameter.lower()=='true':\n",
    "        print(site_name+' - excluding outliers.')\n",
    "        outlier_mask,outlier_cutoff = make_outlier_mask(results_dict['uncorrected']['coverage'],site_name)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        for key in results_dict.keys():\n",
    "            results_dict[key]['coverage'] = results_dict[key]['coverage']*outlier_mask\n",
    "    else:\n",
    "        outlier_cutoff='NA'\n",
    "\n",
    "    #normalize to a mean of 1 and smooth\n",
    "    for key in results_dict.keys():\n",
    "        results_dict[key]['coverage'] = normalize_and_smooth(results_dict[key]['coverage'],site_name,key)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    #get features\n",
    "    for key in results_dict.keys():\n",
    "        results_dict[key]['coverage'] = calculate_features(results_dict[key]['coverage'])\n",
    "\n",
    "    #get metadata\n",
    "    for key in results_dict.keys():\n",
    "        results_dict[key]['coverage']['outlier_cutoff']=outlier_cutoff\n",
    "        results_dict[key]['coverage']['exclude_zero_mappability']=exclude_zero_mappability_parameter \n",
    "        results_dict[key]['coverage']['correction'] = key\n",
    "        results_dict[key]['coverage']['number_of_sites']=len(current_sites)\n",
    "        results_dict[key]['coverage']['site_name']=site_name\n",
    "        results_dict[key]['coverage']['smoothing']=smoothing\n",
    "        results_dict[key]['coverage']['CNA_normalization']=CNA_normalization\n",
    "        results_dict[key]['coverage']['sample']=sample_name    \n",
    "        results_dict[key]['coverage'] = results_dict[key]['coverage'].copy()\n",
    "        \n",
    "    #if saving individual sites, keep the locations\n",
    "    if individual.lower()=='true':\n",
    "        current_sites = current_sites.drop(columns = ['site_name'])\n",
    "        for key in results_dict.keys():\n",
    "            results_dict[key]['coverage'] = results_dict[key]['coverage'].merge(current_sites, left_index=True, right_index=True, validate = 'one_to_one')\n",
    "        \n",
    "    elapsed_time = time.time()-overall_start_time\n",
    "    print(site_name+' merge complete '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "    del(elapsed_time)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    return(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for testing\n",
    "# # for site_name in sites.keys()[1]:\n",
    "# for site_name in [list(sites.keys())[1]]:\n",
    "#     site_name = site_name\n",
    "#     site_file = sites[site_name]\n",
    "#     results = merge_sites([site_name,site_file])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYL1.hg38.10000 processing all 10000 sites\n",
      "LYL1.hg38.10000 (fw/rv/undirected/total): 0/0/10000/10000\n",
      "LYL1.hg38.10000 uncorrected starting fetch 0 min 1 sec\n",
      "LYL1.hg38.10000_uncorrected_chr9_137686907_137696897: 10000 of 10000 intervals done in 0 min 12 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000_uncorrected_fetch_complete: 10000 of 10000 intervals done in 0 min 12 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000 GC_corrected starting fetch 0 min 14 sec\n",
      "LYL1.hg38.10000_GC_corrected_chr9_137686907_137696897: 10000 of 10000 intervals done in 0 min 12 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000_GC_corrected_fetch_complete: 10000 of 10000 intervals done in 0 min 13 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000 GC_map_corrected starting fetch 0 min 28 sec\n",
      "LYL1.hg38.10000_GC_map_corrected_chr9_137686907_137696897: 10000 of 10000 intervals done in 0 min 12 sec, -1 min 60 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000_GC_map_corrected_fetch_complete: 10000 of 10000 intervals done in 0 min 13 sec, -1 min 60 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000 - excluding specified regions.\n",
      "LYL1.hg38.10000 to_exclude starting fetch 0 min 42 sec\n",
      "LYL1.hg38.10000_to_exclude_chr9_137686907_137696897: 10000 of 10000 intervals done in 0 min 8 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000_to_exclude_fetch_complete: 10000 of 10000 intervals done in 0 min 9 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000 - excluding zero mappability.\n",
      "LYL1.hg38.10000 mappabilty starting fetch 0 min 52 sec\n",
      "LYL1.hg38.10000_mappabilty_chr9_137686907_137696897: 10000 of 10000 intervals done in 0 min 9 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000_mappabilty_fetch_complete: 10000 of 10000 intervals done in 0 min 9 sec, 0 min 0 sec remaining , size 0.74 GB\n",
      "LYL1.hg38.10000 - excluding outliers.\n",
      "LYL1.hg38.10000 max_bin_coverage is 71594.0 midpoints\n",
      "LYL1.hg38.10000 masking sites with  > 2510.0 midpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/software/IPython/7.13.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: Mean of empty slice\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/app/software/IPython/7.13.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: Mean of empty slice\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYL1.hg38.10000 uncorrected averaging sites\n",
      "LYL1.hg38.10000 uncorrected smoothing\n",
      "LYL1.hg38.10000 uncorrected correcting for read depth\n",
      "LYL1.hg38.10000 GC_corrected averaging sites\n",
      "LYL1.hg38.10000 GC_corrected smoothing\n",
      "LYL1.hg38.10000 GC_corrected correcting for read depth\n",
      "LYL1.hg38.10000 GC_map_corrected averaging sites\n",
      "LYL1.hg38.10000 GC_map_corrected smoothing\n",
      "LYL1.hg38.10000 GC_map_corrected correcting for read depth\n",
      "LYL1.hg38.10000 merge complete 1 min 8 sec\n",
      "Done_calculating profiles 1 min 8 sec\n"
     ]
    }
   ],
   "source": [
    "#run the analysis \n",
    "to_do_list = [[key,sites[key]] for key in sites.keys()]\n",
    "\n",
    "p = Pool(processes=CPU) #use the specified number of processes\n",
    "results = p.map(merge_sites, to_do_list, 1) #Send only one interval to each processor at a time.\n",
    "\n",
    "elapsed_time = time.time()-overall_start_time\n",
    "print('Done_calculating profiles '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "del(elapsed_time)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['uncorrected','GC_corrected','GC_map_corrected']:\n",
    "    current_results = pd.DataFrame()\n",
    "    for i in range(len(results)):\n",
    "        current_results = current_results.append(results[i][key]['coverage'])\n",
    "    current_out_path = results_sample_dir+'/'+sample_name+'.'+key+'.coverage.fragments.tsv'\n",
    "    current_results.to_csv(current_out_path,sep='\\t', index = False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.plot(save_columns, results[0]['uncorrected']['coverage'][save_columns].mean())\n",
    "# # plt.plot(save_columns, results[0]['GC_corrected']['coverage'][save_columns].mean())\n",
    "# # plt.plot(save_columns, results[0]['GC_map_corrected']['coverage'][save_columns].mean())\n",
    "\n",
    "# for i in range(len(results)):\n",
    "#     df = results[i]['GC_map_corrected']['coverage']\n",
    "#     site_name = df['site_name'].values[0]\n",
    "#     plt.plot(save_columns, df[save_columns].mean(), label= site_name)\n",
    "# plt.legend()\n",
    "# plt.title(sample_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exclude_paths==['none']:\n",
    "    os.remove(tmp_bigWig+\"/excluded_regions.bw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
