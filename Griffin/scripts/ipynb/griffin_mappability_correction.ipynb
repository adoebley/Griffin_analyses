{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pyBigWig\n",
    "import pybedtools\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pysam\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# # bam_path = '/fh/scratch/delete90/ha_g/realigned_bams/cfDNA_deepWGS_hg38/deepWGS_fastq_to_bam_paired_snakemake/results/HD45.ctDNA.WGS.FC19269447/HD45.ctDNA.WGS.FC19269447_recalibrated.bam'\n",
    "# bam_path = '/fh/scratch/delete90/ha_g/EGA/EGAC00001001180_JohnsHopkins/EGAD00001005339/realign_bam_paired_snakemake/results/PGDX16569P_WGS.sorted_processed/PGDX16569P_WGS.sorted_processed_recalibrated.bam'\n",
    "# #bam_path = '/fh/scratch/delete90/ha_g/EGA/EGAC00001001180_JohnsHopkins/EGAD00001005339/realign_bam_paired_snakemake/results/PGDX16570P_WGS.sorted_processed/PGDX16570P_WGS.sorted_processed_recalibrated.bam'\n",
    "# sample_name = bam_path.rsplit('/',1)[1].rsplit('.',3)[0]\n",
    "# output_path = 'test_results/v10.'+bam_path.rsplit('/',1)[1].rsplit('.',3)[0]+'.mappability_bias.txt'\n",
    "# plot_path = 'test_results/v10.'+bam_path.rsplit('/',1)[1].rsplit('.',3)[0]+'.mappability_plot.pdf'\n",
    "\n",
    "# mappability_file='/fh/fast/ha_g/user/adoebley/downloads/UCSC_genome/hg38/k100.Umap.MultiTrackMappability.hg38.bw'\n",
    "\n",
    "# exclude_path = '../../../genome/encode_unified_GRCh38_exclusion_list.bed'\n",
    "# centromere_path = '../../../genome/hg38_centromeres.bed'\n",
    "# gap_path = '../../../genome/hg38_gaps.bed'\n",
    "# patch_path = '../../../genome/hg38_fix_patches.bed'\n",
    "# alternative_haplotype_path = '../../../genome/hg38_alternative_haplotypes.bed'\n",
    "\n",
    "# exclude_paths = [exclude_path,centromere_path,gap_path,patch_path,alternative_haplotype_path]\n",
    "# del(exclude_path,centromere_path,gap_path,patch_path)\n",
    "\n",
    "# chrom_sizes_path = '/fh/fast/ha_g/grp/reference/GRCh38/hg38.standard.chrom.sizes'\n",
    "# map_q = 20\n",
    "# CPU = 4\n",
    "# tmp_dir = 'tmp/'\n",
    "\n",
    "# #default params\n",
    "# sampling_fraction = 0.01 #use 0.01 for actual sampling\n",
    "# log_interval = 1 #minutes between reports\n",
    "# chroms = ['chr'+str(m) for m in np.arange(1,23)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--bam_file', help='path to bam file', required=True)\n",
    "parser.add_argument('--bam_file_name', help='sample name', required=True)\n",
    "parser.add_argument('--output', help='path to the output file', required=True)\n",
    "parser.add_argument('--output_plot', help='path to the output plot', required=True)\n",
    "\n",
    "parser.add_argument('--mappability', help='path to mappability track (bigWig format, ex. k50.Umap.MultiTrackMappability.hg38.bw)', required=True)\n",
    "parser.add_argument('--exclude_paths', help='path to bed files of regions to filter out (excluded regions, centromeres, gaps, patches, alternative haplotypes)', required=True, nargs = '*')\n",
    "\n",
    "parser.add_argument('--chrom_sizes', help='path to chrom sizes file', required=True)\n",
    "parser.add_argument('--map_quality',help='minimum mapping quality', type=int, default=60)\n",
    "parser.add_argument('--CPU',help='CPU available for parallelizing', type = int, required = True)\n",
    "parser.add_argument('--tmp_dir',help='directory for temporary pybedtools output', required = True)\n",
    "\n",
    "#default params\n",
    "parser.add_argument('--sampling_fraction',help='fraction of genomic positions to sample', type = float, default = 0.01)\n",
    "parser.add_argument('--log_interval',help='frequency (in minutes) of progress reports printed in the log', type = float, default = 1)\n",
    "parser.add_argument('--chroms', help='chromosomes to use', nargs='*', default=['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22'])\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "bam_path = args.bam_file\n",
    "sample_name = args.bam_file_name\n",
    "output_path = args.output\n",
    "plot_path = args.output_plot\n",
    "\n",
    "mappability_file = args.mappability\n",
    "exclude_paths = args.exclude_paths\n",
    "\n",
    "chrom_sizes_path = args.chrom_sizes\n",
    "map_q = args.map_quality\n",
    "CPU = args.CPU\n",
    "tmp_dir = args.tmp_dir\n",
    "\n",
    "sampling_fraction = args.sampling_fraction\n",
    "log_interval = args.log_interval\n",
    "chroms = args.chroms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybedtools.set_tempdir(tmp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_read(read):\n",
    "    if read.is_paired==True and read.mapping_quality>=map_q and read.is_duplicate==False and read.is_qcfail==False:\n",
    "        return (True)\n",
    "    else:\n",
    "        return (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(index):\n",
    "    #define the current bin\n",
    "    bin_chrom = sampling_set.iloc[index]['chrom']\n",
    "    bin_start = sampling_set.iloc[index]['start']\n",
    "    bin_end = sampling_set.iloc[index]['end']\n",
    "\n",
    "    #set up to collect outputs\n",
    "    map_frequency = np.zeros(101)\n",
    "    map_reads = np.zeros(101)\n",
    "\n",
    "    #open bam file and mappability file\n",
    "    bam_file = pysam.AlignmentFile(bam_path)\n",
    "    mappability = pyBigWig.open(mappability_file)\n",
    "\n",
    "    #filter out excluded regions\n",
    "    current_bed = pybedtools.BedTool(bin_chrom+' '+str(bin_start)+' '+str(bin_end), from_string = True)\n",
    "    filtered = current_bed.subtract(merged_exclude_regions)\n",
    "    \n",
    "    #if the entire bin is filtered out, return the array of zeros\n",
    "    if len(filtered)==0:\n",
    "        print('interval '+str(index)+' ('+bin_chrom+':'+str(bin_start)+'-'+str(bin_end)+')'+' filtered out')\n",
    "        sys.stdout.flush()\n",
    "        bam_file.close()\n",
    "        return(map_frequency,map_reads)\n",
    "    \n",
    "    #otherwise process the bin\n",
    "    filtered = filtered.to_dataframe()\n",
    "    \n",
    "    for i in range(len(filtered)):\n",
    "        chrom = filtered.iloc[i]['chrom']\n",
    "        start = filtered.iloc[i]['start']\n",
    "        end = filtered.iloc[i]['end']\n",
    "\n",
    "        #get mappability value \n",
    "        map_vals = np.round(mappability.values(chrom,start,end),2)\n",
    "        map_vals = np.nan_to_num(map_vals) #turn nan into 0\n",
    "        map_vals = np.int32(np.round(100*map_vals))\n",
    "\n",
    "        counts = bam_file.count_coverage(chrom,start,end,read_callback=check_read)\n",
    "        counts = np.array(counts).sum(axis=0)\n",
    "\n",
    "        for j in np.arange(0,len(map_vals),step):\n",
    "            if counts[j] < upper_limit: #exclude outliers\n",
    "                map_frequency[map_vals[j]]+=1 \n",
    "                map_reads[map_vals[j]]+=counts[j]\n",
    "        del(i,j)\n",
    "\n",
    "    time_elapsed = time.time()-start_time\n",
    "    expected_time = (time_elapsed/(index+1))*len(sampling_set)\n",
    "    remaining_time = expected_time-time_elapsed\n",
    "\n",
    "    if (index+1)%20==0:\n",
    "        print('interval '+str(index)+' ('+bin_chrom+':'+str(bin_start)+'-'+str(bin_end)+')'+': '+\n",
    "              str(index+1) +' of '+ str(len(sampling_set))+' intervals done in '+\n",
    "              str(np.int32(time_elapsed/60))+' min, '+str(np.int32(remaining_time/60))+' min remaining')\n",
    "    sys.stdout.flush()\n",
    "    bam_file.close()\n",
    "    \n",
    "    return(map_frequency,map_reads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up an array of positions to sample\n",
    "print('splitting into '+str(CPU)+' arrays for parallel processing')\n",
    "sys.stdout.flush()\n",
    "\n",
    "chrom_sizes = pd.read_csv(chrom_sizes_path, sep='\\t', header=None)\n",
    "chrom_sizes = chrom_sizes.set_index(0)\n",
    "step = np.int32(np.round(1/sampling_fraction))\n",
    "\n",
    "#get the sampling set\n",
    "sampling_set = pd.DataFrame()\n",
    "bin_length = 5000000 #max length of bin to fetch\n",
    "\n",
    "for chrom in chroms:\n",
    "    max_val = chrom_sizes.loc[chrom].values[0]\n",
    "    chr_sampling_set = pd.DataFrame(pd.Series(np.arange(0,max_val,bin_length),name = 'start'))\n",
    "    chr_sampling_set['chrom'] = chrom\n",
    "    chr_sampling_set['end'] = chr_sampling_set['start']+bin_length\n",
    "    #replace the last bin end with the max value\n",
    "    chr_sampling_set['end'] = np.where(chr_sampling_set['end']>max_val,max_val,chr_sampling_set['end'])\n",
    "    sampling_set = sampling_set.append(chr_sampling_set,ignore_index = True)\n",
    "    del(max_val,chr_sampling_set)\n",
    "sampling_set = sampling_set[['chrom','start','end']]\n",
    "print('sampling', len(sampling_set),'bins',bin_length, 'bp per bin')\n",
    "sys.stdout.flush()\n",
    "\n",
    "#get the excluded regions\n",
    "merged_exclude_regions = pybedtools.BedTool('\\n', from_string=True)\n",
    "for path in exclude_paths:\n",
    "    print('excluding:',path)\n",
    "    current_regions = pybedtools.BedTool(path)\n",
    "    merged_exclude_regions = merged_exclude_regions.cat(current_regions)    \n",
    "    del(current_regions)\n",
    "    \n",
    "#extend merged regions to the nearest step\n",
    "df = merged_exclude_regions.to_dataframe()\n",
    "df['start'] = np.floor(df['start']/step)*step\n",
    "df['start'] = df['start'].astype(int)\n",
    "df['end'] = np.ceil(df['end']/step)*step\n",
    "df['end'] = df['end'].astype(int)\n",
    "df = df[df['chrom'].isin(chroms)]\n",
    "merged_exclude_regions = pybedtools.BedTool.from_dataframe(df)\n",
    "merged_exclude_regions = merged_exclude_regions.merge()\n",
    "del(df)\n",
    "excluded_length = merged_exclude_regions.total_coverage()\n",
    "print('Excluding',len(merged_exclude_regions),'regions covering',excluded_length,'bp')\n",
    "print(str(chrom_sizes.loc[chroms].sum().values[0]-excluded_length)+' of '+str(chrom_sizes.loc[chroms].sum().values[0])+' bp retained')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample the read coverage distribution to identify extreme outliers\n",
    "print('Sampling read coverage distribution')\n",
    "sys.stdout.flush()\n",
    "start_time = time.time()\n",
    "\n",
    "random_sample = sampling_set.sample(2000, random_state = 100, replace=True) #sample some extra sites\n",
    "\n",
    "sampled_counts = []\n",
    "bam_file = pysam.AlignmentFile(bam_path)\n",
    "for i in range(len(random_sample)):\n",
    "    chrom = random_sample.iloc[i]['chrom']\n",
    "    \n",
    "    #get a random position within the current bin\n",
    "    rng = np.random.default_rng(100)#set the seed to 100\n",
    "    start = random_sample.iloc[i]['start']\n",
    "    end = random_sample.iloc[i]['end']\n",
    "    position = rng.choice(np.arange(start,end,step))\n",
    "\n",
    "    current_interval = pybedtools.BedTool(chrom+' '+str(np.int32(position))+' '+str(np.int32(position)+1), from_string=True)\n",
    "    \n",
    "    #check if the interval is excluded\n",
    "    if len(merged_exclude_regions.all_hits(current_interval[0]))==0:  \n",
    "        count = bam_file.count(chrom,position,position+1,read_callback=check_read)\n",
    "        sampled_counts.append(count)\n",
    "        del(count)\n",
    "    elif len(merged_exclude_regions.all_hits(current_interval[0]))==1: \n",
    "        pass\n",
    "    else:\n",
    "        print('Excluded regions were not be properly merged')\n",
    "    if len(sampled_counts)==1000:\n",
    "        break\n",
    "    if i%200==0:\n",
    "        print(i,'of 1000 sites sampled')\n",
    "        sys.stdout.flush()\n",
    "    os.remove(current_interval.fn)\n",
    "bam_file.close()\n",
    "\n",
    "\n",
    "sampled_counts = pd.DataFrame(pd.Series(sampled_counts, name = 'read_count'))\n",
    "\n",
    "upper_limit = sampled_counts['read_count'].mean() + sampled_counts['read_count'].std(ddof=0)*5\n",
    "print('Sampled',len(sampled_counts),'positions in '+str(np.round(time.time()-start_time))+' seconds')\n",
    "print('Upper limit for non outlier coverage: '+str(np.round(upper_limit,2)))\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the read depth and outliers\n",
    "fig,ax = plt.subplots(1)\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "bins = max(sampled_counts['read_count'])-min(sampled_counts['read_count'])+1\n",
    "sampled_counts['read_count'].hist(ax = ax, bins = bins, range = (0,max(sampled_counts['read_count'])+1),  \n",
    "                                  align = 'left', rwidth = 0.8, color = 'tab:blue', alpha = 0.2, label = 'outliers (>'+str(np.round(upper_limit,2))+')')\n",
    "xlim = ax.get_xlim()\n",
    "ax.set_xlim(xlim[0],xlim[1])\n",
    "ax.set_ylabel('number of positions')\n",
    "\n",
    "non_outliers = sampled_counts[(sampled_counts['read_count']<=upper_limit)]\n",
    "non_outliers['read_count'].hist(ax = ax, bins = bins, range = (0,max(sampled_counts['read_count'])+1),  \n",
    "                                align = 'left', rwidth = 0.8, color = 'tab:blue', label = 'non-outliers')\n",
    "\n",
    "#density plot\n",
    "sampled_counts['read_count'].plot.kde(ax = ax2, color = 'tab:orange', label = 'density')\n",
    "ylim2 = ax2.get_ylim()\n",
    "ax2.set_ylim(0,ylim2[1])\n",
    "\n",
    "ax.set_xlabel('number of reads')\n",
    "ax.set_title(sample_name+'\\n'+'histogram of read depth')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig(plot_path.rsplit('.',1)[0]+'.read_coverage_distribution.pdf')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "del(sampled_counts,random_sample,non_outliers,bins,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for testing\n",
    "\n",
    "# start_time = time.time()\n",
    "# for index in range(len(sampling_set)):\n",
    "#     map_frequency,map_reads = process_segment(index)\n",
    "#     break\n",
    "    \n",
    "# #for testing\n",
    "# results_df = pd.DataFrame(pd.Series(np.arange(0,1.01,.01), name = 'mappability'))\n",
    "# results_df['frequency'] = map_frequency\n",
    "# results_df['coverage_count'] = map_reads\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process arrays in parallel\n",
    "start_time = time.time()\n",
    "indexes = np.arange(len(sampling_set))\n",
    "\n",
    "p = Pool(processes=CPU) #use the available CPU\n",
    "results = p.map(process_segment, indexes, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add up the results from the parallel jobs \n",
    "map_frequency = np.zeros(101)\n",
    "map_reads = np.zeros(101)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    current_map_frequency,current_map_reads = results[i]    \n",
    "    map_frequency = map_frequency+current_map_frequency\n",
    "    map_reads = map_reads+current_map_reads\n",
    "    \n",
    "results_df = pd.DataFrame(pd.Series(np.arange(0,101,1), name = 'mappable_percent'))\n",
    "results_df['frequency'] = map_frequency\n",
    "results_df['read_count'] = map_reads\n",
    "results_df = results_df[results_df['frequency']!=0]\n",
    "results_df['mean_map_bias'] = results_df['read_count'] / results_df['frequency']\n",
    "results_df['mean_map_bias'] = results_df['mean_map_bias']/np.nanmean(results_df['mean_map_bias'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the individual bins and perform lowess smoothing\n",
    "map_frequency_df = pd.DataFrame(np.array(results)[:,0,:]).copy()\n",
    "map_reads_df = pd.DataFrame(np.array(results)[:,1,:]).copy()\n",
    "map_bias_df = map_reads_df/map_frequency_df\n",
    "print('done converting to dataframe')\n",
    "\n",
    "map_bias_list = map_bias_df.unstack().reset_index(level=0).copy()\n",
    "map_bias_list = map_bias_list.rename(columns = {'level_0':'mappable_percent',0:'mappability_bias'})\n",
    "map_bias_list = map_bias_list[~(map_bias_list['mappability_bias'].isnull())]\n",
    "print('done reshaping')\n",
    "\n",
    "regression_line = lowess(endog = map_bias_list['mappability_bias'],exog = map_bias_list['mappable_percent'], \n",
    "       frac = 0.04, is_sorted = False, return_sorted = False)\n",
    "\n",
    "map_bias_list['smoothed_map_bias'] = regression_line\n",
    "\n",
    "print('done smoothing')\n",
    "\n",
    "regression_plot_vals = map_bias_list[['mappable_percent','smoothed_map_bias']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "mean_val = np.nanmean(regression_plot_vals['smoothed_map_bias'])\n",
    "\n",
    "regression_plot_vals['smoothed_map_bias'] = regression_plot_vals['smoothed_map_bias']/mean_val\n",
    "\n",
    "map_bias_list['mappability_bias'] = map_bias_list['mappability_bias']/mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (6,4))\n",
    "random_sample = map_bias_list.sample(2000, random_state = 100)\n",
    "\n",
    "ax.scatter(random_sample['mappable_percent'],random_sample['mappability_bias'], alpha = 0.2, label = 'bins')\n",
    "ax.scatter(results_df['mappable_percent'].astype(float), results_df['mean_map_bias'], color = 'black', label = 'mean map bias')\n",
    "ax.plot(regression_plot_vals['mappable_percent'],regression_plot_vals['smoothed_map_bias'],label = 'smoothed map bias')\n",
    "\n",
    "ax.legend(loc = 'upper left', bbox_to_anchor = [1,1])\n",
    "ax.set_xlabel('mappability (percent)')\n",
    "ax.set_ylabel('coverage bias')\n",
    "ax.set_title(sample_name+'\\n mappability_bias')\n",
    "ax.axhline(0.5, dashes = [2,2], color = 'grey', alpha = 0.5)\n",
    "ax.axhline(1.0, dashes = [2,2], color = 'grey', alpha = 0.5)\n",
    "ax.axhline(1.5, dashes = [2,2], color = 'grey', alpha = 0.5)\n",
    "ax.set_ylim(-0.05,3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(plot_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_export = results_df.merge(regression_plot_vals, on = 'mappable_percent')\n",
    "to_export.to_csv(output_path,sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete temporary files opened by the current session\n",
    "pybedtools.cleanup(verbose=False, remove_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
