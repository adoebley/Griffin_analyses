{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import pybedtools\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml \n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# ##sample specific params for testing\n",
    "# sample_name = 'HD45.ctDNA.WGS.FC19269447'\n",
    "# bam_path = '/fh/scratch/delete90/ha_g/realigned_bams/cfDNA_deepWGS_hg38/deepWGS_fastq_to_bam_paired_snakemake/results/HD45.ctDNA.WGS.FC19269447/HD45.ctDNA.WGS.FC19269447_recalibrated.bam'\n",
    "# GC_bias_path = '../../../GC_correction/deepWGS_GC_and_mappability_correction/results/GC_bias/HD45.ctDNA.WGS.FC19269447.GC_bias.txt'\n",
    "# mappability_bias_path = '../../../GC_correction/deepWGS_GC_and_mappability_correction/results/mappability_bias/HD45.ctDNA.WGS.FC19269447.mappability_bias.txt'\n",
    "\n",
    "# # sample_name = 'MBC_1041_1_ULP'\n",
    "# # bam_path = '../../../MBC_copy_bams/bam_file_copies/MBC_1041_1_ULP_recalibrated.bam'\n",
    "# # GC_bias_path = '../../../GC_correction/MBC_ULP_GC_and_mappability_correction/results/GC_bias/MBC_1041_1_ULP.GC_bias.txt'\n",
    "# # mappability_bias_path = '../../../GC_correction/MBC_ULP_GC_and_mappability_correction/results/mappability_bias/MBC_1041_1_ULP.mappability_bias.txt'\n",
    "\n",
    "\n",
    "# tmp_dir = 'tmp'\n",
    "\n",
    "# ref_seq_path = '/fh/fast/ha_g/grp/reference/GRCh38/GRCh38.fa'\n",
    "# mappability_bw='../../../genome/k100.Umap.MultiTrackMappability.hg38.bw'\n",
    "# chrom_sizes_path = '/fh/fast/ha_g/grp/reference/GRCh38/hg38.standard.chrom.sizes'\n",
    "\n",
    "# # #additional params for testing\n",
    "# # sites_yaml = '../../../MBC/ATAC_nucleosome_profiling_with_CNA_correction/config/sites.yaml'\n",
    "# sites_yaml = '/fh/fast/ha_g/user/adoebley/projects/griffin_revisions_1/MBC/ATAC_nucleosome_profiling/config/sites.yaml'\n",
    "# griffin_scripts_dir = '../'\n",
    "\n",
    "# chrom_column = 'Chrom'\n",
    "# position_column = 'position'\n",
    "# strand_column = 'Strand'\n",
    "# chroms = ['chr'+str(m) for m in np.arange(1,23)]\n",
    "\n",
    "# norm_window = [-5000, 5000] \n",
    "# # norm_window = [-5000, 5000] \n",
    "# sz_range = [100, 200]\n",
    "# map_q = 20\n",
    "\n",
    "# # number_of_sites = 10000\n",
    "# # sort_by = 'Chrom'\n",
    "# # #sort_by = 'peak.count'\n",
    "# # ascending = 'False'\n",
    "\n",
    "# number_of_sites = 'none'\n",
    "# sort_by = 'none'\n",
    "# ascending = 'none'\n",
    "\n",
    "# CPU = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--sample_name', help='name of sample', required=True)\n",
    "parser.add_argument('--bam', help='bam file', required=True)\n",
    "parser.add_argument('--GC_bias', help='GC bias info from griffin_GC_bias', required=True)\n",
    "parser.add_argument('--mappability_bias', help='mappability bias info from griffin_mappability_bias', required=True)\n",
    "\n",
    "parser.add_argument('--tmp_dir', help = 'directory for temporary outputs (may be large)', required=True)\n",
    "\n",
    "parser.add_argument('--reference_genome',help = 'path to the reference genome',required=True)\n",
    "parser.add_argument('--mappability_bw',help = 'bigWig file of genome wide mappability scores',required=True)\n",
    "parser.add_argument('--chrom_sizes_path', help='path to chrom sizes file', required=True)\n",
    "\n",
    "parser.add_argument('--sites_yaml', help='.bed file of sites', required=True)\n",
    "parser.add_argument('--griffin_scripts_dir', help='path/to/scripts/', required=True)\n",
    "\n",
    "parser.add_argument('--chrom_column',help='name of column containing chromosome number', default='Chrom')\n",
    "parser.add_argument('--position_column',help='name of column containing chromosome position', default='Chrom')\n",
    "parser.add_argument('--strand_column',help='name of column containing the strand (+ or -)', default='Strand')\n",
    "parser.add_argument('--chroms', help='chroms to include when selecting sites', nargs='*', default=['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22'])\n",
    "\n",
    "parser.add_argument('--norm_window',help='start and end of the window to be used for normalization',nargs=2, type=int, default=(-5000,5000))\n",
    "parser.add_argument('--size_range',help='acceptable size range for fragments (to filter out genomic contamination)',nargs=2, type=int, default=(0,500))\n",
    "parser.add_argument('--map_quality',help='minimum mapping quality', type=int, default=60)\n",
    "\n",
    "parser.add_argument('--number_of_sites',help='number of sites to analyze', default='NA')\n",
    "parser.add_argument('--sort_by',help='how to select the sites to analyze', default='none')\n",
    "parser.add_argument('--ascending',help='whether to sort in ascending or descending order when selecting sites', default='NA')\n",
    "\n",
    "parser.add_argument('--CPU',help='cpu available for parallelizing', type = int, required = True)\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "sample_name = args.sample_name\n",
    "bam_path = args.bam\n",
    "GC_bias_path = args.GC_bias\n",
    "mappability_bias_path = args.mappability_bias\n",
    "\n",
    "tmp_dir = args.tmp_dir \n",
    "\n",
    "ref_seq_path = args.reference_genome\n",
    "mappability_bw = args.mappability_bw\n",
    "chrom_sizes_path = args.chrom_sizes_path\n",
    "\n",
    "sites_yaml=args.sites_yaml\n",
    "griffin_scripts_dir = args.griffin_scripts_dir\n",
    "\n",
    "chrom_column = args.chrom_column\n",
    "position_column=args.position_column\n",
    "strand_column=args.strand_column\n",
    "chroms = args.chroms\n",
    "\n",
    "norm_window =args.norm_window\n",
    "sz_range=args.size_range\n",
    "map_q=args.map_quality\n",
    "\n",
    "number_of_sites=args.number_of_sites\n",
    "sort_by=args.sort_by\n",
    "ascending=args.ascending\n",
    "\n",
    "CPU = args.CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print parameters for easy troubleshooting\n",
    "if ascending.lower()=='false':\n",
    "    ascending=False\n",
    "elif ascending.lower()=='true':\n",
    "    ascending=True\n",
    "else:\n",
    "    ascending='none'\n",
    "    \n",
    "print('\\nparameters:')\n",
    "\n",
    "print('\\tsample_name = \"'+sample_name+'\"')\n",
    "print('\\tbam_path = \"'+bam_path+'\"')\n",
    "print('\\tGC_bias_path = \"'+GC_bias_path+'\"')\n",
    "print('\\tmappability_bias_path = \"'+mappability_bias_path+'\"')\n",
    "\n",
    "print('\\ttmp_dir = \"'+tmp_dir+'\"')\n",
    "\n",
    "print('\\tref_seq_path = \"'+ref_seq_path+'\"')\n",
    "print('\\tmappability_bw = \"'+mappability_bw+'\"')\n",
    "print('\\tchrom_sizes_path = \"'+chrom_sizes_path+'\"')\n",
    "\n",
    "print('\\tsites_yaml = \"'+os.path.abspath(sites_yaml)+'\"')\n",
    "print('\\tgriffin_scripts_dir = \"'+griffin_scripts_dir+'\"')\n",
    "\n",
    "print('\\tchrom_column = \"'+chrom_column+'\"')\n",
    "print('\\tposition_column = \"'+position_column+'\"')\n",
    "print('\\tstrand_column = \"'+strand_column+'\"')\n",
    "print('\\tchroms = ',chroms)\n",
    "\n",
    "print('\\tnorm_window = ', norm_window)\n",
    "print('\\tsz_range =',sz_range)\n",
    "print('\\tmap_q =',map_q)\n",
    "\n",
    "print('\\tnumber_of_sites = \"'+str(number_of_sites)+'\"')\n",
    "print('\\tsort_by = \"'+sort_by+'\"')\n",
    "print('\\tascending = \"'+str(ascending)+'\"')\n",
    "\n",
    "print('\\tCPU =',CPU)\n",
    "print('\\n')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define global parameters and open global files\n",
    "########################################\n",
    "#GET GC BIAS\n",
    "########################################\n",
    "#open the GC_bias file \n",
    "GC_bias = pd.read_csv(GC_bias_path, sep='\\t')\n",
    "\n",
    "#get rid of extremely low GC bias values\n",
    "#these fragments will now be excluded \n",
    "#these fragments are extremely rare so it is difficult to get a good estimate of GC bias\n",
    "GC_bias['smoothed_GC_bias'] = np.where(GC_bias['smoothed_GC_bias']<0.05,np.nan,GC_bias['smoothed_GC_bias'])\n",
    "\n",
    "GC_bias = GC_bias[['length','num_GC','smoothed_GC_bias']]\n",
    "GC_bias = GC_bias.set_index(['num_GC','length']).unstack()\n",
    "\n",
    "#convert to a dictionary\n",
    "GC_bias = GC_bias.to_dict()\n",
    "\n",
    "#get rid of values where the num_GC is greater than the length (included due to the way I made the dict)\n",
    "GC_bias2 = {}\n",
    "for key in GC_bias.keys():\n",
    "    length = key[1]\n",
    "    GC_bias2[length] = {}\n",
    "    for num_GC in range(0,length+1):\n",
    "        bias = GC_bias[key][num_GC]\n",
    "        GC_bias2[length][num_GC]=bias\n",
    "GC_bias = GC_bias2 \n",
    "del(GC_bias2)\n",
    "\n",
    "########################################\n",
    "#GET mappability bias\n",
    "########################################\n",
    "mappability_bias = pd.read_csv(mappability_bias_path, sep='\\t')\n",
    "mappability_bias['mappable_percent'] = np.round(mappability_bias['mappable_percent']).astype(int) #convert indexes to integers\n",
    "mappability_bias = mappability_bias[['mappable_percent','smoothed_map_bias']].set_index('mappable_percent').to_dict()['smoothed_map_bias']\n",
    "print('using smoothed map bias')\n",
    "def closest_key(dictionary, i):\n",
    "    sorted_keys=np.array(sorted(dictionary.keys()))\n",
    "    idx = (np.abs(sorted_keys - i)).argmin() \n",
    "    closest_key=sorted_keys[idx]\n",
    "    return(closest_key)\n",
    "\n",
    "#depending on the read length, some values might be missing, add them\n",
    "for i in range(0,101):\n",
    "    if i in mappability_bias:\n",
    "        pass\n",
    "    else:\n",
    "        mappability_bias[i] = mappability_bias[closest_key(mappability_bias,i)]\n",
    "        print('adding mappabilty',i,'to dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snakemake should create these folders, but if not using the snakemake, this is needed\n",
    "tmp_sample_dir = tmp_dir+'/'+sample_name\n",
    "if not os.path.exists(tmp_sample_dir): \n",
    "    os.mkdir(tmp_sample_dir)\n",
    "\n",
    "tmp_pybedtools = tmp_sample_dir+'/tmp_pybedtools'\n",
    "if not os.path.exists(tmp_pybedtools): \n",
    "    os.mkdir(tmp_pybedtools)\n",
    "pybedtools.set_tempdir(tmp_pybedtools)\n",
    "\n",
    "tmp_bigWig = tmp_sample_dir+'/tmp_bigWig'\n",
    "if not os.path.exists(tmp_bigWig): \n",
    "    os.mkdir(tmp_bigWig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the griffin scripts\n",
    "sys.path.insert(0, griffin_scripts_dir)\n",
    "import griffin_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the site_lists\n",
    "with open(sites_yaml,'r') as f:\n",
    "    sites = yaml.safe_load(f)\n",
    "sites = sites['site_lists']\n",
    "\n",
    "all_sites = pd.DataFrame()\n",
    "for site_name in sites.keys():\n",
    "    site_file = sites[site_name]\n",
    "    current_sites = griffin_functions.import_and_filter_sites(site_name,site_file,strand_column,chrom_column,position_column,chroms,ascending,sort_by,number_of_sites)\n",
    "    all_sites = all_sites.append(current_sites, ignore_index=True).copy()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of bp to fetch upstream and downstream of the site\n",
    "upstream_bp = norm_window[0]-sz_range[0] #this should be negative\n",
    "downstream_bp = norm_window[1]+sz_range[0] #this should be positive\n",
    "all_sites = griffin_functions.define_fetch_interval('Total sites',all_sites,chrom_column,position_column,\n",
    "                                                    chroms,chrom_sizes_path,upstream_bp,downstream_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pybedtools and merge overlapping segments\n",
    "start_time = time.time()\n",
    "all_sites_bed = pybedtools.BedTool.from_dataframe(all_sites[[chrom_column,'fetch_start','fetch_end']])\n",
    "all_sites_bed = all_sites_bed.sort()\n",
    "all_sites_bed = all_sites_bed.merge()\n",
    "print('Intervals to fetch:\\t'+str(len(all_sites_bed)))\n",
    "print('Total bp to fetch:\\t'+str(all_sites_bed.total_coverage()))\n",
    "sys.stdout.flush()\n",
    "\n",
    "#split the long intervals \n",
    "to_fetch = all_sites_bed.to_dataframe()\n",
    "to_fetch['length'] = to_fetch['end']-to_fetch['start']\n",
    "\n",
    "print('Max fetch length: '+str(to_fetch['length'].max())+' bp')\n",
    "\n",
    "to_fetch = to_fetch[['chrom','start','end']]\n",
    "to_fetch = to_fetch.sort_values(by=['chrom','start']).reset_index(drop=True)\n",
    "to_fetch = to_fetch.reset_index() #add an index column\n",
    "split_len = pybedtools.BedTool.from_dataframe(to_fetch[['chrom','start','end']]).total_coverage()\n",
    "\n",
    "# print('Intervals to fetch (after splitting long intervals):\\t'+str(len(to_fetch)))\n",
    "# print('Total bp to fetch (after splitting long intervals):\\t'+str(split_len))\n",
    "sys.stdout.flush()\n",
    "pybedtools.cleanup(verbose=False, remove_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fragments(input_list):\n",
    "    i,chrom,start,end = input_list\n",
    "    #open the bam file for each pool worker (otherwise individual pool workers can close it)\n",
    "    bam_file = pysam.AlignmentFile(bam_path)\n",
    "    \n",
    "    #open the ref seq\n",
    "    ref_seq=pysam.FastaFile(ref_seq_path)\n",
    "    mappability = pyBigWig.open(mappability_bw)    \n",
    "    \n",
    "    #make dicts to hold the fetched positions\n",
    "    columns = np.arange(start,end,1)\n",
    "    cov_dict={m:0 for m in columns} \n",
    "    GC_cov_dict={m:0 for m in columns} \n",
    "    GC_map_cov_dict={m:0 for m in columns} \n",
    "    \n",
    "    #fetch reads\n",
    "    fetched=bam_file.fetch(contig=chrom, start=start, stop=end) #fetch reads that map to the region of interest\n",
    "                \n",
    "    ########################\n",
    "    #count coverage\n",
    "    ########################\n",
    "    for read in fetched:\n",
    "        #filter out reads\n",
    "        if abs(read.template_length)>=sz_range[0] and abs(read.template_length)<=sz_range[1] \\\n",
    "        and read.is_paired==True and read.mapping_quality>=map_q and read.is_duplicate==False and read.is_qcfail==False:\n",
    "            #only use fw reads with positive fragment lengths (negative indicates an abnormal pair)\n",
    "            #all paired end reads have a fw and rv read so we don't need the rv read to find the midpoint.\n",
    "            if read.is_reverse==False and read.template_length>0:\n",
    "                fragment_start = read.reference_start #for fw read, read start is fragment start\n",
    "                fragment_end = read.reference_start+read.template_length\n",
    "                midpoint = int(np.floor((fragment_start+fragment_end)/2))\n",
    "                                \n",
    "                #count the GC content\n",
    "                fragment_seq = ref_seq.fetch(read.reference_name,fragment_start,fragment_end)\n",
    "                fragment_seq = np.array(list(fragment_seq.upper()))\n",
    "                fragment_seq[np.isin(fragment_seq, ['A','T','W'])] = 0\n",
    "                fragment_seq[np.isin(fragment_seq, ['C','G','S'])] = 1\n",
    "                rng = np.random.default_rng(fragment_start)\n",
    "                fragment_seq[np.isin(fragment_seq, ['N','R','Y','K','M','B','D','H','V'])] = rng.integers(2, size=len(fragment_seq[np.isin(fragment_seq, ['N','R','Y','K','M','B','D','H','V'])])) #random integer in range(2) (i.e. 0 or 1)\n",
    "                fragment_seq = fragment_seq.astype(float)\n",
    "    \n",
    "                #find the two read locations for mappability correction\n",
    "                fw_read_map = mappability.values(chrom,read.reference_start,read.reference_start+read.reference_length)\n",
    "                fw_read_map = np.mean(np.nan_to_num(fw_read_map)) #replace any nan with zero and take the mean\n",
    "\n",
    "                rv_read_map = mappability.values(chrom,read.reference_start+read.template_length-read.reference_length,read.reference_start+read.template_length)\n",
    "                rv_read_map = np.mean(np.nan_to_num(rv_read_map)) #replace any nan with zero and take the mean\n",
    "                                \n",
    "                #check that the site is in the window          \n",
    "                if midpoint>=start and midpoint<end:\n",
    "                    #count the fragment\n",
    "                    cov_dict[midpoint]+=1\n",
    "\n",
    "                    ##get the GC bias\n",
    "                    read_GC_content = sum(fragment_seq)\n",
    "                    read_GC_bias = GC_bias[abs(read.template_length)][read_GC_content]\n",
    "                    \n",
    "                    #get the mappability bias\n",
    "                    read_map = np.int32(np.round(100*(fw_read_map+rv_read_map)/2))\n",
    "                    read_map_bias = mappability_bias[read_map]\n",
    "\n",
    "                    #count the fragment weighted by GC bias\n",
    "                    if not np.isnan(read_GC_bias):\n",
    "                        GC_cov_dict[midpoint]+=(1/read_GC_bias)\n",
    "                        GC_map_cov_dict[midpoint]+=(1/read_GC_bias)*(1/read_map_bias)\n",
    "                    #print(read_GC_bias,read_map,read_map_bias)\n",
    "                else: #if fragment doesn't fully overlap\n",
    "                    #print('end outside')\n",
    "                    continue\n",
    "                del(read,midpoint,fw_read_map,rv_read_map,fragment_seq)\n",
    "                \n",
    "            else:\n",
    "                #print('reverse',read.is_reverse)\n",
    "                continue\n",
    "                \n",
    "    output = pd.DataFrame(pd.Series(cov_dict, name = 'uncorrected'))\n",
    "    output['GC_corrected'] = pd.Series(GC_cov_dict)\n",
    "    output['GC_map_corrected'] = pd.Series(GC_map_cov_dict)\n",
    "    output = output[output['uncorrected']>0] #don't waste memory on positions with no coverage\n",
    "    output['chrom'] = chrom\n",
    "    output = output.reset_index().rename(columns = {'index':'position'})\n",
    "    output['uncorrected'] = output['uncorrected'].astype(float)\n",
    "    output['GC_corrected'] = np.round(output['GC_corrected'],5)\n",
    "    output['GC_map_corrected'] = np.round(output['GC_map_corrected'],5)\n",
    "\n",
    "    bam_file.close()\n",
    "    ref_seq.close()\n",
    "    mappability.close()\n",
    "    \n",
    "    if (i+1)%1000==0:\n",
    "        printout = griffin_functions.progress_report([chrom,start,end],'intervals',start_time,time.time(),i,len(to_fetch))\n",
    "        print(printout)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the analysis \n",
    "print('Starting fetch')\n",
    "sys.stdout.flush()\n",
    "start_time = time.time()\n",
    "\n",
    "p = Pool(processes=CPU) #use the specified number of processes\n",
    "results = p.map(collect_fragments, to_fetch.values, 1) #Send only one interval to each processor at a time.\n",
    "\n",
    "elapsed_time = time.time()-overall_start_time\n",
    "print('Done with fetch '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "del(elapsed_time)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting export')\n",
    "sys.stdout.flush()\n",
    "start_time = time.time()\n",
    "\n",
    "chrom_sizes = pd.read_csv(chrom_sizes_path, sep='\\t', header=None)\n",
    "chrom_sizes = chrom_sizes[chrom_sizes[0].isin(chroms)]\n",
    "\n",
    "uncorrected_bw = pyBigWig.open(tmp_bigWig+\"/\"+sample_name+\".uncorrected.bw\", \"w\")\n",
    "GC_bw = pyBigWig.open(tmp_bigWig+\"/\"+sample_name+\".GC_corrected.bw\", \"w\")\n",
    "GC_map_bw = pyBigWig.open(tmp_bigWig+\"/\"+sample_name+\".GC_map_corrected.bw\", \"w\")\n",
    "\n",
    "uncorrected_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "GC_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "GC_map_bw.addHeader([(a,b) for a,b in chrom_sizes.values])\n",
    "\n",
    "for i in range(len(results)):\n",
    "    current = results[i]\n",
    "    if len(current)>0:\n",
    "        if np.nansum(current['uncorrected'])>0:\n",
    "            uncorrected_bw.addEntries(list(current['chrom']), list(current['position']), ends = list(current['position']+1), values = list(current['uncorrected']))  \n",
    "        else:\n",
    "            print('no uncorrected reads:')\n",
    "            print(current)\n",
    "            \n",
    "        if np.nansum(current['GC_corrected'])>0:\n",
    "            GC_bw.addEntries(list(current['chrom']), list(current['position']), ends = list(current['position']+1), values = list(current['GC_corrected'])) \n",
    "        else:\n",
    "            print('no GC corrected reads:')\n",
    "            print(current)\n",
    "            \n",
    "        if np.nansum(current['GC_map_corrected'])>0:   \n",
    "            GC_map_bw.addEntries(list(current['chrom']), list(current['position']), ends = list(current['position']+1), values = list(current['GC_map_corrected']))  \n",
    "        else:\n",
    "            print('no GC map corrected reads:')\n",
    "            print(current)\n",
    "            \n",
    "    if (i+1)%10000==0 and len(current)>0:\n",
    "        printout = griffin_functions.progress_report(list(current.iloc[0][['chrom','position']]),'intervals',start_time,time.time(),i,len(results))\n",
    "        print(printout)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "uncorrected_bw.close()\n",
    "GC_bw.close()\n",
    "GC_map_bw.close()\n",
    "\n",
    "elapsed_time = time.time()-overall_start_time\n",
    "print('Done with export '+str(int(np.floor(elapsed_time/60)))+' min '+str(int(np.round(elapsed_time%60)))+' sec')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
